# MOOC INRIA / UCA : Python 3 : des fondamentaux aux concepts avancés du langage

## Arnaud Legout et Thierry Parmentelat 

## Transcriptions des videos de la semaine 8 : Programmation asynchrone - asyncio


# W8-S1 Programmation asynchrone


Bonjour à tous !


Cette semaine, nous allons parler de **programmation asynchrone**. C'est un sujet qui est assez innovant puisque ça met en oeuvre un paradigme de programmation qui est plutôt différent de ce à quoi on est habitué. C'est aussi un sujet qui devient de plus en plus prégnant puisque dans un monde qui est tourné complètement vers le réseau. Ce sont des types d'applications qui deviennent de plus en plus largement utilisées, et pour ceux qui connaissent, vous trouverez des similitudes avec ce qu'on peut trouver dans des langages comme C# et JavaScript, enfin toute une famille de langages.

Pour revenir à Python, nous, on s'est donné aujourd'hui un budget de temps d'une semaine, donc on n'aura pas évidemment le temps d'aller très, très loin. Moi, mon objectif, c'est de faire en sorte que vous sachiez que ça existe, de vous donner les bases, de vous expliquer la magie qu'il y a là derrière de façon à ce que vous puissiez commencer à jouer avec ce paradigme qui est à nouveau très intéressant. Donc je m'adresse à un public qui est avancé, clairement, voire très avancé ; ça risque de demander un petit investissement de votre part mais si vous êtes de près ou de loin impliqués dans des domaines de réseaux, je pense que c'est très intéressant pour vous d'investiguer et de voir ce que vous pouvez tirer de ce paradigme.

Dans cette première séquence, je vais expliciter ce que j'entends par **programmation asynchrone**. Pour commencer, on va faire une différence entre deux grandes familles d'applications. Vous avez les applications qui sont gourmandes en cycles, donc pensez à tout ce qui peut être calcul mathématique. Ce sont des applications dont la vitesse va être limitée par la disponibilité de cycles de calcul. Dans une deuxième famille d'applications, vous pouvez penser à toutes les applications qui au contraire ne vont pas être limitées par le processeur, mais par l'environnement extérieur, par la vitesse à laquelle les événements du type entrées/sorties sont susceptibles d'arriver à l'application. Donc c'est principalement tout ce qui peut être lié à la programmation réseau, aux interfaces utilisateur par exemple. Donc bien entendu, pour une application donnée, vous allez avoir des morceaux qui sont "CPU intensive" et d'autres qui sont "I/O intensive", mais en tout cas, nous, ce qui nous concerne cette semaine, on s'adresse uniquement à ce qui est "I/O intensive", qui est la partie pour laquelle la programmation asynchrone a été conçue.

Pour bien illustrer ça, je vais prendre l'exemple, tout à fait typique, d'un robot - pensez au robot Google - dont le travail, ça va être d'aller chercher des pages web en grande quantité. Pour commencer, je vais prendre le cas hyper simple où je m'intéresse à une URL, et je simplifie au maximum, pour dire que le robot principalement va envoyer une requête à un serveur, ce serveur va prendre un certain temps, va lui renvoyer une réponse. Donc le *workflow*, je vous l'indique de cette façon. À nouveau, c'est extrêmement simple ; le point qui est important c'est qu'entre les deux événements qui sont "j'envoie une requête" et "je reçois une requête", en gros, le processeur ne fait rien, et on va dire que c'est du temps perdu. Tant qu'il n'y a qu'une seule requête à traiter ça n'est pas un problème, mais évidemment, si j'ai plusieurs requêtes à faire, ce qui est bien entendu le cas dans la réalité, avec un paradigme de programmation traditionnelle, ce que je peux faire c'est de les faire en séquence et là, c'est encore pire puisque j'attends la première, j'attends la deuxième, donc si j'ai une centaine de milliers de requêtes à faire, je vais être extrêmement inefficace.

Les outils dont je vais parler cette semaine adressent ce problème-là. On cherche à tirer le meilleur profit possible des cycles, mais on cherche aussi à le faire d'une manière qui ne défigure pas la logique qui est derrière. Ce qu'on a envie de faire, c'est d'écrire un code qui dit "moi, ce que je veux faire, c'est envoyer une requête, attendre qu'elle revienne et en faire quelque chose", sans avoir à défigurer le code, à changer l'architecture ou à modifier la structure du code. Donc c'est ça que nous allons voir dans la suite.

Pour commencer, je voudrais faire quelques rappels sur le fonctionnement des ordinateurs et des systèmes d'exploitation, qu'on appelle *operating systems* en anglais. Donc la première abstraction qu'offre un *operating system* pour les applications, c'est ce qu'on appelle un *process*, un processus. L'idée, c'est assez simple, on vous offre un mécanisme d'isolation complète ; vous allez pouvoir mettre dans un process naturellement un morceau de code, une zone de mémoire qui est une mémoire virtuelle et qui n'est pas du tout partagée avec les autres processus. C'est le principe. Et puis bien entendu, à titre accessoire, il y a ce qu'on appelle une pile, c'est-à-dire que c'est là où on va garder la trace des différents appels de fonction, et puis un pointeur qui dit où on en est dans le code.

Pour revenir à mon problème de robot, je pourrais penser à utiliser les process en me disant : je vais créer un processus par requête. Effectivement, sur le plan du principe ça fonctionne. J'ai du parallélisme, je vais tirer profit du processeur de manière optimale. C'est l'*operating system* qui va passer la main d'un process à l'autre. Je vais régler le problème que j'indiquais de tirer profit de mes cycles.

Par contre, j'ai un gros problème, c'est que d'abord j'ai trop d'isolation dans le sens où quand j'ai plein de requêtes à faire, la plupart du temps, il y a une logique : je vais chercher une première page, ça va me donner des URL que je vais chercher ensuite, donc j'ai besoin que les différents morceaux de mon application se causent, donc l'isolation dans ce cas précis, ce n'est pas du tout favorable.

Un deuxième problème, c'est que ce n'est pas du tout un paradigme qui va me permettre de passer à l'échelle, je peux espérer faire une dizaine, une centaine peut-être de requêtes en parallèle. Bref, ce n'est pas bon

Ce qui s'est passé ensuite, c'est que avec l'arrivée de machines multi-processeurs, *multi-core*, on s'est dit - là, c'était aussi plutôt pour les *CPU intensive*, on va dire on va inventer un autre truc, on va dire qu'un process, il n'y a pas qu'un seul *thread*, il y en a plusieurs. L'idée est la suivante, c'est très simple, on reste dans un environnement isolé, la mémoire est toujours isolée, mais mon process va pouvoir avoir en gros plusieurs *threads*, donc plusieurs tâches en même temps. Alors, ça, c'est un paradigme qui est un petit peu plus intéressant pour nous, je signale que c'est disponible en Python, c'est largement utilisé, il y a un module exprès qui s'appelle `threading`. Par contre, c'est un système qui est d'une utilisation un petit peu plus délicate, dont je vous dirai un mot tout de suite.

En gros, ça introduit des problèmes de synchronisation entre les *threads* et de sections critiques - je vais en dire un mot. En terme de passage à l'échelle, c'est plus favorable pour nous, on peut imaginer passer peut-être à l'échelle de plusieurs milliers, mais on va voir que ce n'est pas forcément suffisant et avec les technologies dont je vais vous parler, on va pouvoir faire beaucoup mieux que ça.

Pour bien voir le problème qu'on a avec les *threads*, si vous imaginez maintenant que j'implémente mon robot de deux requêtes avec deux *threads* différents, je vais bien avoir les deux phases qui consistent à envoyer les requêtes à mes deux serveurs - je vais les voir dans ma tête, conceptuellement, elles sont en même temps. En fait, ce qui ce passe, c'est que ça n'est pas moi qui ai le contrôle sur les moments où l'on va passer la main au premier ou au deuxième. En fait, cet arbitrage-là, cette orchestration-là, ils sont faits par le *scheduler* de *threads*, sur lequel je n'ai absolument pas la main et qui n'a pas de connaissance a priori de la logique de mon programme.
C'est surtout ça qui va nous permettre d'améliorer la lisibilité de notre code, c'est que, avec les paradigmes que je vais vous présenter, vous aurez le contrôle de à quel moment exactement vous acceptez de redonner la main, ou à quel moment vous voulez absolument la garder parce que c'est important en terme de section critique par exemple.

Maintenant, pour changer de sujet complètement, pour se placer maintenant d'un point de vue qui est beaucoup plus applicatif, il y a dans un certain nombre de langages et de paradigmes, une autre approche complètement, qui s'appelle les *callbacks*. Alors les *callbacks*, c'est l'idée qu'on va associer à un événement une fonction à exécuter. Donc c'est relativement simple sur le plan du concept ; on n'est plus du tout du côté de l'*operating system*, on est vraiment du côté applicatif. Par contre, ça a un gros souci, les *callbacks*, c'est que ça vous amène à couper votre code en tout petits morceaux, je vous en montrerai un exemple tout à l'heure, et ça induit une logique qui devient difficile à suivre.

Alors, **asyncio**, le paradigme sur lequel on va parler toute cette semaine, se fixe en gros comme objectif d'apporter une solution alternative aux *process*, aux *threads* et aux *callbacks*. L'idée, ça va être de construire cette fois-ci la notion de **coroutines** qui s'exécutent en parallèle, donc jusque-là, ça ressemble un peu à un *thread*, sauf que ce sont des choses qui vont s'exécuter dans un seul *thread* de la machine et l'orchestration de ces différents *pseudo-threads* ou *threads applicatifs*, on pourrait les appeler ainsi, elle est faite avec la connaissance du code et c'est ce qui va nous permettre d'avoir un contrôle très fin sur la commutation entre les différents morceaux de notre programme. La conséquence de ça, c'est qu'on va pouvoir un code qui est beaucoup plus lisible, - à nouveau, je vais vous en donner un exemple tout de suite - en gardant le contrôle sur les changements de contexte sans avoir à saucissonner notre code en morceaux, et en plus, cerise sur le gâteau, ça va nous permettre d'améliorer encore plus la scalabilité, on va pouvoir envisager d'héberger des dizaines de milliers, voire des centaines de milliers, de requêtes en parallèle.

Pour bien matérialiser cela sur un exemple assez simple, je vous montre le code en version *callback* à gauche, d'un protocole qui s'appelle le protocole ping-pong, qui est très classique dans les co-réseaux. Le protocole ping-pong, c'est très simple, le client envoie un message au serveur qui lui renvoie le message. C'est vraiment très simple. Donc à droite vous avez la version, alors dans un langage imaginaire, ça ressemble un peu à du Python, mais en gros ce serait ça que vous écririez sous forme de coroutine, vous dites : je suis le serveur, j'attends ce que m'envoie le client, je le lui renvoie, je vide mon buffer, et je ferme la connexion. Donc là on a la logique de ce petit protocole qui est tout à fait lisible.

Si vous regardez à gauche la même chose écrite dans un langage basé *callback*, ça ressemble à du JavaScript clairement, bon, vous voyez qu'on passe son temps à dire: sur un événement, j'arme une fonction qui elle-même va de nouveau armer un événement sur une fonction... C'est ce qu'on appelle le *callback hell* dans la littérature. C'est très pratique parce que ça permet notamment dans les applications web, de faire des applis qui sont très réactives, donc ça donne de très bonnes performances mais par contre c'est vraiment au prix d'une... on défigure complètement le code et ce n'est pas du tout souhaitable.

Donc, avec `asyncio` pour conclure, on va arriver à faire notre petit exercice en ayant une utilisation optimale des cycles mais avec un code qui est complètement lisible et en plus avec un paradigme qui est complètement *scalable*. C'est surtout de ça dont on va parler dans le reste de la semaine.

À bientôt !


# W8-S2 Quelques exemples simples


Dans la vidéo précédente, je vous ai introduit **asyncio**, je vous ai expliqué dans quel contexte ça se plaçait. Comme c'était très abstrait, je vais maintenant être un peu plus concret ; on va voir, sur des tout petits exemples, à quoi ça ressemble, juste histoire d'introduire les concepts de base.

Ça se présente sous la forme d'une librairie qui s'appelle `asyncio`, sans grande surprise. Le concept de base, c'est la **coroutine**. Je vais vous montrer comment on définit une coroutine. Alors, ça se présente sous la forme, ça ressemble un petit peu à une fonction sauf qu'il y a `async` qui est défini au préalable, ça veut dire ceci est une coroutine. On écrit un code qui ressemble assez à une fonction traditionnelle sauf que, à certains endroits, j'utilise une instruction qui s'appelle `await`. Cette instruction, à ce stade de l'exposé, on va dire c'est une instruction qui dit : attention, je suis en train d'invoquer quelque chose qui est susceptible de s'interrompre. Je signifie de cette façon que je suis disposé à rendre la main au *scheduler*, dont je vous ai parlé la fois précédente.

La coroutine que vous avez sous les yeux ne fait pas grand chose, elle écrit un message du début, elle attend une demi-seconde en rendant la main, elle affiche un deuxième message disant qu'elle est au milieu, elle re-attend une seconde et puis, à la fin, elle affiche un message de fin. J'évalue ce bout de code Python.

Ça ne fait rien, c'est la même chose que si j'avais défini une fonction, presque. Alors, ce n'est pas tout à fait quand même la même chose parce que si je l'évalue, je me rends compte que c'est une fonction d'accord, mais par contre, si j'essaie de l'appeler, et bien, en gros, il ne se passe rien. Alors, il ne se passe rien, ça me renvoie un objet, qui est une coroutine, et donc ça me donne l'occasion de commencer à introduire la différence entre ce qui est une **fonction coroutine** et l'**objet coroutine** qui est rendu par la fonction. C'est un mécanisme qui est assez proche des générateurs ; on aura l'occasion d'en reparler, mais pour l'instant, ce que je veux vous montrer, c'est comment on s'en sert.

La façon la plus simple d'exécuter ce code, pour moi, et bien c'est de l'exécuter au travers d'une **boucle d'événements**. Ça tombe bien, dans `asyncio`, il y a la notion de boucle d'événements donc je vais appeler la fonction `get_event_loop` qui me renvoie une boucle d'événements, et sur cette boucle, je vais pouvoir envoyer une méthode qui s'appelle `run_until_complete` ; je vais lui passer une coroutine, en fait, plus précisément, un objet coroutine, et le travail de cette méthode, ça va être de faire avancer ma coroutine jusquà ce qu'elle soit terminée. Voilà, on va voir ce que ça donne. Jusque là, pas de surprise, je veux simplement vous montrer qu'avec la boucle, je peux enfin exécuter le code que j'ai écrit.

Alors, là où c'est le plus intéressant, c'est si j'en fais plusieurs à la fois. Ça se présenterait sous cette façon-là ; au lieu de passer à `run_until_complete` seulement une coroutine, je vais lui en passer deux. Alors en fait, je triche ; techniquement, je vais lui envoyer un seul objet coroutine que j'ai construit en utilisant `gather`. On aura l'occasion de reparler de tout ça ; dans ce tout premier exemple, ce que je veux vous montrer, c'est la façon la plus simple d'avoir deux choses qui se déroulent en parallèle.

Donc dans ce contexte-là, ce que vous voyez, c'est que je vais effectivement faire les deux débuts, à peu près en même temps, les deux milieux à peu près en même temps, donc c'est à peu près ce que je voulais faire et vous avez dans cette figure quelque chose qui illustre bien l'utilisation que je fais du temps. Le *scheduler*, donc ma boucle en l'occurrence, va choisir une des deux coroutines, là je n'ai pas de contrôle sur celle qui va commencer puisque j'ai décidé qu'elles commençaient en même temps, je ne l'ai pas dit. J'imagine que c'est `run1`. On va d'abord exécuter le petit bout de code qui est avant le `await` du `run1`. Au moment où on voit le `await`, on va changer de contexte, la boucle sait qu'elle peut changer de contexte, elle va faire le début de `run2`, et caetera.

Alors, j'attire votre attention, là, j'ai mis `await`, on verra dans la suite qu'en fait j'aurais dû mettre `yield`, mais pour l'instant, à ce stade de l'exposé, je préfère être simple.

Donc c'est le modèle mental, c'est l'idée que la boucle est le *scheduler* qui va passer la main d'une coroutine à l'autre. Je vous montre tout de suite un exemple de ce qu'il ne faut pas faire.

Si, au lieu d'écrire `morceaux` comme je l'avais fait tout à l'heure, je l'écris de cette façon-là ; je l'ai appelé `famine` parce que, entre les deux messages de milieu et de fin, au lieu de faire un `asyncio.sleep`, je fais un `time.sleep` donc une fonction normale, qui n'est pas avertie du paradigme du tout, donc ce que ça veut dire, c'est que je ne vais pas rendre la main à la boucle. Du coup, si je l'évalue de la même façon que tout à l'heure, je vais avoir les deux débuts qui arrivent en même temps, mais une fois qu'une de mes deux coroutines prendra la main, elle va la garder, elle va faire milieu puis fin, évidemment.

Ce qu'on peut voir, c'est la façon dont ça se passe dans le temps. La grosse période verte, c'est la période pendant laquelle je n'ai pas rendu le temps. Donc bien entendu, dans la vraie vie, ce sont des calculs ; je fais beaucoup de `sleep` dans toute cette présentation mais imaginez qu'à la place il y ait des calculs. Donc cette façon de ne pas rendre la main, il ne faut pas la faire, il faut vraiment être obsédé par le fait de ne pas garder la main disons plus que, je n'en sais rien, 10 millisecondes, 100 millisecondes au grand maximum.

Donc en guise de conclusion, je vous ai montré comment on pouvait créer des **coroutines** avec `async def` et non pas `await def` ; je vous ai montré également comment on pouvait créer une **boucle d'événements** pour orchestrer plusieurs coroutines ; et je vous ai également montré qu'une coroutine, ça pouvait appeler d'autres coroutines en faisant simplement `await`. Et on a également vu, quand on a appelé `time.sleep`, qu'une coroutine, ça peut tout à fait appeler une fonction traditionnelle, qu'on va appeler une **fonction synchrone** ; simplement, il faut faire attention de ne pas se lancer dans un calcul qui soit trop long.

Donc voilà une toute première introduction pour complètement défricher le sujet ; on va creuser tout ça dans les vidéos suivantes.

À bientôt !


# W8-S3 asyncio : historique et écosystème


Dans la vidéo précédente, nous avons vu des exemples d'utilisation d'`asyncio` peut-être un petit peu moins abstraits que ce qu'on avait pu voir dans la toute première. Maintenant je vais vous donner un petit aperçu de l'historique et de ce qui est disponible, puis vous donner quelques indications pour savoir peut-être vers quelle direction les choses sont susceptibles de se diriger.

Pour commencer, un petit historique : s'agissant de python3, la syntaxe on va dire moderne avec des `async` et des `await`, qu'on a vue tout à l'heure, c'est une syntaxe qui date de *python3.5*. 

Je vous signale que dans la version 3.4, il y a eu une première introduction de `asyncio` à titre expérimental, avec une syntaxe différente que celle que je vous indique à l'écran. Ce n'est plus beaucoup utilisé, en tout cas, le nouveau code bien sûr ne l'utilise pas mais vous avez une petite chance de le trouver dans du code encore si vous lisez les librairies tierces.

S'agissant de *python2*, je voulais simplement signaler qu'en fait de manière assez étonnante, c'est en 2006 déjà qu'on a introduit dans le langage ce qui est vraiment la clé, ce qui a déverrouillé le tout c'est-à-dire le moment où on a introduit la possibilité d'envoyer des messages aux générateurs, je reviendrai là-dessus longuement dans les vidéos suivantes. Mais tout ça pour vous dire que depuis 2006, il y a déjà un certain nombre de communautés qui se sont intéressées à ce paradigme, qui ont défriché le terrain. J'en ai signalées 3 principalement qui sont assez fréquemment citées dans toute la littérature autour de `asyncio`.

Ce que je voudrais aussi faire maintenant c'est une distinction très nette entre ce qui appartient au langage lui-même de ce qui appartient à la librairie `asyncio`. Pourquoi ? Parce que la librairie `asyncio` est dans la librairie standard maintenant, donc ça veut dire qu'elle va être maintenue de manière indéfinie, elle n'est plus expérimentale d'ailleurs, mais il n'est pas complètement exclu que à terme ce soit, pas remplacé, mais que il y ait d'autres librairies du même genre qui apparaissent. Donc c'est assez important de bien voir ce qui appartient au langage lui-même par rapport à ce qui appartient à la librairie parce que, si vous voulez utiliser une autre librairie, il est possible que vous ayez à utiliser d'autres mécanismes et donc c'est important qu'on voit bien la différence.

Donc ce qui appartient au langage, ce sera là-dessus que je vais me focaliser pour commencer, c'est la notion de **coroutine**, donc les `async def` et `await`, les **itérateurs asynchrones**, on va en parler dans une vidéo, les **compréhensions asynchrones**, les **context managers asynchrones**, en gros, tous les mécanismes qui sont vraiment internes au langage. Par contre, ce qui appartient à la librairie, c'est la **boucle d'événements** elle-même, les **mécanismes de synchronisation** - alors là, à nouveau on verra ça en détail dans une autre vidéo - mais il y a des mécanismes de synchronisation qui ressemblent assez à ce que vous pourriez trouver dans une librairie de *thread*. Il y a également un certain nombre de fonctionnalités qui s'adressent à la gestion de processus externes. Et puis il y a tout ce qui est lié, en gros, aux vraies entrées/sorties de l'*operating system*, c'est-à-dire tout ce qui est création de connexion, lecture de fichier, et caetera. Donc c'est important de bien voir la différence entre les deux, on aura l'occasion d'approfondir.

Je voudrais dire aussi un mot sur en gros, si vous vous posez la question de savoir si le paradigme est utilisable en production, je vous mets quelques éléments là-dessus. Donc je viens de dire que ça a été déclaré comme étant stable, ce n'est plus une *feature* expérimentale, ce qui veut donc dire qu'elle sera maintenue pendant longtemps. En matière de performance, c'est correct, il y a un certain nombre d'indicateurs qui disent que c'est certainement améliorable mais c'est très correct, en tout cas, c'est bien meilleur que d'utiliser des *threads*. Et à l'heure actuelle il y a une offre assez vaste, je pense qu'on peut dire que pratiquement tous les protocoles réseaux, toutes les bases de données également en sont couvertes, puisque les bases de données présentent le même type de mécanisme essentiellement que l'interface réseau.

Par contre, on peut dire que ce n'est pas totalement parfait non plus. La première des choses, c'est que c'est un paradigme qui est contagieux, ça veut dire que si vous prenez une application qui a été écrite dans une approche traditionnelle et que vous décidez de la passer à cette approche asyncio, vous allez vous rendre compte que vous allez avoir de plus en plus de code qui va passer d'une fonction traditionnelle à une fonction de type coroutine. C'est lié à la façon dont le paradigme est construit. Alors, ce n'est pas forcément un problème, mais c'est vrai que c'est plus agréable de commencer de scratch et d'écrire une application qui est conforme à tout ça que de tripoter une application qui n'a pas été conçue pour. Ça, c'est quand même un truc.

Enfin, je voudrais vous signaler un certain nombre d'autres expériences on va dire ; ce sont - alors, je vous ai signalé les 3 grands joueurs qui étaient avant `asyncio` là, je vous signale les 3 grands joueurs qui seront peut-être après `asyncio`. Donc, `uvloop`, ça va probablement être le remplaçant de la boucle d'événements d'`asyncio`, c'est quelque chose qui est plus efficace mais la transition, si ça devait être adopté, la transition serait essentiellement transparente pour l'utilisateur. Ça, ce n'est pas un vrai souci.

Par contre, les deux autres, elles arrivent avec un paradigme qui est un petit peu différent, c'est-à-dire que au lieu de considérer que c'est la boucle d'événements le principal, le citoyen de premier rang, on renverse les choses et on considère que c'est la coroutine l'objet de base et je pense que c'est vraiment la bonne façon de prendre les choses.

Donc ça veut dire que sur la durée, il est possible, comme je l'indiquais tout à l'heure, que la librairie `asyncio` soit remplacée ou qu'il y ait d'autres alternatives. Ce qui n'est pas grave, parce que c'est arrivé plein de fois dans la librairie standard, que l'on mette un morceau de code puis qu'on en mette une deuxième version, on conserve toujours le précédent. Mais voilà, si vous voulez vous projeter dans l'avenir, c'est important de savoir que ce n'est peut-être pas la tout dernière version de la librairie.

Voilà, je vous retrouve dans la prochaine vidéo.

À bientôt !


# W8-S4 Extensions asynchrones du langage


Dans cette vidéo, nous allons faire un focus sur les **extensions asynchrones** du langage.

Et nous allons prendre comme prétexte d'aller chercher quatre URL, d'abord en séquence, donc je me définis mes quatre URL, et dans la version séquentielle, que je lance puisque ça prend un petit peu de temps, je vous montre que, évidemment comme on s'y attendait et comme on l'a bien compris, on est allé chercher les requêtes une par une et que par conséquent, je vais avoir un temps total de l'ordre de en l'occurrence onze secondes. Bon, très bien, c'était ce qu'on avait escompté.

Dans une version asynchrone, je vais utiliser une librairie qui s'appelle `aiohttp`. C'est l'équivalent, en gros, d'une *stack* http mais qui aurait été écrite dans la version `asyncio`. Et je me définis une coroutine que j'ai appelée `fetch`, qui va matérialiser la logique qui consiste à d'abord créer une connexion http, puis à l'intérieur de cette session http je vais envoyer un `get`, et puis, à partir de ce `get`, je vais pouvoir lire la réponse qui va au `get`. L'intérêt pour moi de découper en morceaux par rapport à la première version séquentielle qui était vraiment macroscopique, c'est de bien vous montrer que je vais pouvoir faire les choses essentiellement en parallèle.

Donc je me définis ma coroutine. Vous pouvez remarquer l'utilisation d'un **context manager asynchrone**, que j'utilise avec `async with`. Alors, qu'est-ce que c'est qu'un **context manager asynchrone**, c'est tout simplement comme un *context manager*, sauf qu'au lieu de définir les méthodes spéciales `__enter__` et `__exit__`, c'est `__aenter__` et `__aexit__` qui doivent renvoyer des *awaitables*, on aura l'occasion d'en reparler dans la prochaine vidéo.

C'est défini dans la PEP492 mais nous, ce qui nous intéresse en particulier c'est tout simplement que le code qui est dans *enter/exit*, au lieu que ce soit du code synchrone, c'est du code asynchrone. C'est la seule différence ce qui fait que la boucle, au moment où elle va exécuter le `async with`, au lieu d'appeler de manière bloquante le code de *enter/exit*, elle va le faire de manière asynchrone. C'est la seule différence, c'est très simple.

Donc pour vous montrer ce que ça donne je me définis maintenant, pour pouvoir passer ça à ma boucle, je vais définir une coroutine, dont je n'aurais pas strictement besoin mais c'est pour les besoins de faire quelque chose d'assez simple. Donc vous voyez ici que je vais prendre les quatre coroutines `fetch(url)`, je vais les passer à `gather`, alors j'utilise une `* arg` puisque `gather` s'attend à avoir des arguments qui sont passés individuellement, et maintenant je lance ma boucle et vous allez voir que les quatre URL vont se déclencher en même temps et que je vais avoir, en gros, comme temps total, le temps de la plus longue au lieu d'avoir à attendre la somme de toutes les requêtes.

Dans la continuité de ce qu'on vient de voir sur les *context managers* asynchrones, il y a aussi la notion d'**itérations asynchrones**. De la même façon que vous avez `async with`, on va faire maintenant `async for`. On aura aussi la possibilité de faire des **compréhensions asynchrones**, qui est un *feature* qui est arrivé un petit peu plus tard, mais qui est disponible dans la 3.6 également.

Donc je vous montre une version légèrement améliorée de `fetch2()`. Donc après avoir utilisé le *context manager* pour gérer une session puis pour gérer la requête, je vais maintenant utiliser un `async for` sur les lignes qui reviennent en réponse, ce qui va me permettre de vraiment intercepter chaque ligne individuellement, et tout ça, dans un mode asynchrone. Donc ce que ça donnerait si je le faisais de cette façon-là, c'est exactement le même *boilerplate* qu'avant. Le détail n'a pas vraiment beaucoup d'importance, c'est vraiment pour vous montrer que je reçois les lignes, je suis capable d'intercepter l'événement qui consiste à "je retourne une ligne". Évidemment, ça vient par gros blocs parce que la requête revient en gros paquet de lignes, mais je suis capable vraiment d'intercepter à des niveaux extrêmement fins le protocole, je vais descendre très profond. Tout ça en n'ayant pratiquement pas défiguré mon code, j'ai vraiment fait presque comme si c'était du code synchrone ; au lieu de faire `for line in reponse` je fais un `async for line`.

Donc pour résumer tout ce qu'on a vu jusqu'à maintenant, on a vu qu'on pouvait créer des fonctions coroutines avec `async def` ; on a vu que ces fonctions coroutines retournaient un objet coroutine, qu'on pouvait utiliser essentiellement avec `await`. On a vu également que depuis une coroutine on pouvait appeler une autre coroutine en faisant un `await`. Par contre, il n'est pas possible, et le langage le sanctionne par une erreur de syntaxe, il n'est pas possible de faire `await` dans une fonction synchrone, pour la raison que ça n'aurait pas de sens et on cherche à éviter des erreurs, disons, les plus grossières. Ça n'était pas le cas à une époque lointaine, on a décidé maintenant que c'était carrément strictement interdit.

Nous avons vu à l'instant la notion de *context manager* asynchrone, de boucle asynchrone ; bon, les compréhensions asynchrones se présentent à peu près exactement de la même façon : des crochets avec `async for`, c'est exactement le même principe. Et nous avons vu très brièvement la boucle d'événements, ça, on va le creuser un petit peu plus tard.

Donc je vous donne rendez-vous dans la prochaine vidéo où on va expliciter la notion de **awaitable**, qui est assez centrale dans le paysage.

À bientôt !


# W8-S5 Coroutines et awaitables


J'ai déjà eu l'occasion de mentionner la notion de **awaitable**, donc je vais dans cette vidéo expliciter ce qu'est un **awaitable** ; et je vais me placer dans un contexte qui est légèrement différent. Jusqu'ici je vous ai fait un exposé qui était orienté utilisateur de la librairie : on crée des boucles, on crée des coroutines ; maintenant, je vais me mettre dans la position de quelqu'un qui a envie d'écrire une boucle. Pas pour vous apprendre à faire une boucle mais pour vous expliquer les mécanismes de base qui sont à l'oeuvre là derrière.

Donc dans toute la vidéo je vais bien faire attention de ne jamais importer `asyncio` ; je reste avec des mécanismes qui sont uniquement dans le langage et je fais un petit peu comme si j'avais envie d'expérimenter avec ma propre boucle.

Pour commencer, le protocole **awaitable** c'est un protocole parmi d'autres ; on a déjà eu l'occasion de rencontrer le protocole *itérable*, le protocole *context manager*, et d'autres. Le protocole *awaitable*, c'est le protocole qui décrit sur quels objets on a le droit de faire `await`, naturellement. Il repose sur la méthode spéciale `__await__` sans surprise. Et on en a déjà vu un exemple, c'est l'objet coroutine.

Je vais vous montrer tout de suite comment on peut se créer son propre *awaitable*. Donc ce que me spécifie le protocole c'est que la méthode spéciale `__await__` doit renvoyer un itérateur. Un itérateur, c'est - pensez dans votre tête - un générateur. Donc ça va être une fonction qui va renvoyer des `yield` principalement, et on va voir que c'est ce `yield` qui va servir de point clé dans le séquencement de nos coroutines entre elles.

Je me définis une classe `awaitable`, je fournis la méthode spéciale `__await__` qui ressemble à un générateur et qui va faire une fois `yield`. Ce que ça veut dire, c'est que ce *awaitable* va faire en gros son travail en deux fois. Je définis la classe `awaitable`, je me définis une coroutine qui `await` le *awaitable*, naturellement, je calcule l'objet coroutine que me retourne la coroutine, et maintenant, encore une fois je me mets dans la position de la boucle, je vais envoyer à cet objet coroutine la méthode `send`, et par cette méthode `send` je vais lui envoyer l'objet `None`. Alors, pourquoi `None`, on aura le temps d'en reparler. Donc ce qui se passe, c'est que je reçois en retour ce qu'a *yieldé* le générateur. Donc, ça, c'est la mécanique de base la plus simple sur un *awaitable* qui ne fait presque rien.

Alors maintenant, je vais prendre un *awaitable* un tout petit peu plus compliqué, qui au lieu de faire son travail avec un `yield` va faire son travail avec deux `yield`. Donc j'évalue la classe, je refais le même *boilerplate* qui consiste à calculer un objet coroutine pour pouvoir lui envoyer la méthode `send`. Si je lui envoie la méthode `send` une fois, je récupère `yield 1`. Si j'envoie la méthode `send` deux fois, je récupère `yield 2`. Et si j'essaie de lui envoyer une troisième fois, ce qui va se passer, je vais récupérer l'exception `StopIteration`, donc exactement comme si j'étais en train de faire une boucle for, et la valeur qui est associée à l'exception `StopIteration`, c'est la valeur de retour d'`awaitable`. Donc l'*awaitable* qui fait son travail essentiellement en trois fois, si on veut, je vais pouvoir séquencer ces trois différentes séquences en lui envoyant trois fois `send`.

Donc maintenant vous avez compris, si j'ai une boucle et je veux faire avancer deux tâches en parallèle, je vais créer deux objets coroutines comme tout à l'heure, et je vais pouvoir choisir - là, je choisis arbitrairement - de faire avancer une fois l'une une fois l'autre donc si j'envoie la méthode `send` d'abord à `coro1` et à `coro2`, j'ai les premières étapes de chacune qui sont faites, pareil si je fais ça, et de la même façon que tout à l'heure, si je leur envoie une troisième fois la méthode `send`, je vais récupérer tout simplement la valeur de retour.

Donc maintenant vous voyez que j'ai en tant que boucle la possibilité de choisir quelle est la coroutine que je veux faire avancer et de séquencer absolument tout ce que je veux dans l'ordre que je veux. Vous avez la mécanique de base, presque, pour écrire une boucle.

Je vais maintenant vous montrer un exemple un tout petit peu plus compliqué, où on va voir les mécaniques qui sont à l'oeuvre lorsque les coroutines s'appellent les unes les autres, qui était quelque chose qu'on n'a encore pas bien vu avec les exemples que je vous ai montrés.

Je commence par me définir une classe `w1` qui est un `awaitable`, qui me renvoie 1. Lorsque je vais faire `await` d'un objet de ce type, je vais recevoir 1. Et je vais l'utiliser de la façon suivante : j'appelle un scénario qui, principalement, appelle une fonction coroutine `w4` ; `w4` appelle `w3` et lui ajoute 1, qui appelle `w2` qui ajoute 1, et `w2` va essentiellement me faire `await` sur deux instances successives de `w1`. La raison d'être des marqueurs, c'est uniquement pour qu'on comprenne bien ce qui se passe. Alors, avec ceci en place, je vais pouvoir envoyer mon message une fois à la coroutine. Je vais pouvoir l'envoyer une troisième fois, une deuxième fois pardon, et c'est à la troisième fois que je vais récupérer mon résultat.

Donc mon point, c'est de vous montrer que le vrai moment où l'exécution s'arrête, ce n'est pas le moment où je fais un `await` à proprement parler, c'est le moment où je fais un `yield`. Donc ceci, c'est pour préciser ce que j'avais pu dire en première approximation dans les vidéos d'introduction. Je vais vous montrer ce que ça donne sur une animation. Dans ce scénario, comment ça se passe ? Je commence avec une pile vide. Donc au moment où j'envoie le premier `send` je vais démarrer à proprement parler l'exécution, je vais invoquer `w4`, je vais mettre dans ma pile `w3` puisque `w4` appelle `w3` qui va empiler `w2`, qui va empiler `w1`, et là, à ce stade-là, j'arrive au `yield`. Donc c'est à ce moment-là et à ce moment-là seulement qu'on redonne le contrôle à la pile, le résultat du `yield` est passé à la boucle, qui peut faire absolument ce qu'elle veut, puis, un moment plus tard, elle va nous envoyer le deuxième `send` donc on va continuer l'exécution, on va finir l'exécution de `w1`, on va retourner à `w2`, puis, à ce moment-là, `w2` crée une deuxième instance de `w1`, donc on empile `w1`, on arrive au `yield`, et, au moment où on arrive au `yield`, de nouveau, on rend la main à la boucle, et c'est seulement quand on fait le 3° `send` qu'on va s'amuser à tout dépiler entièrement la pile, et ce résultat 4 sera transmis à la boucle sous la forme de `StopIteration(4)`.

Jusqu'à maintenant, nous avons vu des scénarios dans lesquels on arrivait à faire passer de l'information de la coroutine à la boucle, puisque, lorsque la coroutine fait `yield x`, ce `x` est propagé à la boucle comme résultat du `send`. Maintenant, en fait, cette communication, elle fonctionne dans les deux sens donc on va voir ça ensemble. Je me définis un `awaitable` qui est presque exactement le même que celui qu'on a vu tout à l'heure sauf que je récupère le résultat de l'instruction `yield`, je l'imprime. Comme vous vous en doutez sans doute, ce que je vais arriver à récupérer comme ça, c'est ce qui est envoyé par `send`. Donc on voit ça maintenant ensemble.

La première fois, je ne peux pas faire autrement que d'envoyer `None`, alors, pourquoi je ne peux pas faire autrement que d'envoyer `None` ? C'est qu'en fait je m'adresse à un `yield` lorsque je fais `send` de quelque chose, ça va être le résultat d'un `yield`, sauf que là, cette coroutine n'a pas encore démarré, je n'ai pas encore de pile, il n'y a pas de `yield`, donc je ne peux pas faire autre chose que `send(None)`. La deuxième fois par contre, j'envoie `"message"` qui va se faire récupérer comme résultat de `yield`, et donc, nous avons ici la coroutine qui affiche `"message"`. Et si je regarde maintenant d'un point de vue plus abstrait en faisant une représentation de la pile, je vois que j'ai établi une communication dans les deux sens entre la boucle et la coroutine, et que quand, en gros, pour faire une image très abstraite, si je fais `y = send(x)`, ça veut dire que `x` sera le résultat du `y` et en fait, au coup suivant, le `y` sera le résultat du `send`.

En conclusion de cette vidéo, nous avons vu ce qu'était le **protocole awaitable** et nous nous en sommes servis pour illustrer la communication qui marche dans les deux sens entre la boucle d'un côté, la coroutine de l'autre, ce qui d'un côté correspond au `send` va correspondre de l'autre au `yield`, et qu'on a réussi à faire tout ça sans utiliser à aucun moment la librairie `asyncio`. On n'a utilisé que des mécanismes propres au langage, et c'est ça qui sert à développer des boucles comme la librairie `asyncio`.



# W8-S6 Boucles d'événements



Dans les vidéos précédentes, nous avons vu les mécanismes qui sont dans le langage lui-même. Je vous rappelle qu'on avait vu les mécanismes utilisateurs, définir des coroutines ; nous avons vu les mécanismes plus experts pour commencer à voir comment on fait implémenter les boucles. Maintenant nous allons voir le contenu de la librairie `asyncio`, et pour ça, nous allons commencer par la boucle d'événements.

Donc je vais importer `asyncio`. Avant de commencer, je vais utiliser dans cette vidéo un certain nombre d'utilitaires, en fait un tout petit utilitaire très simple, j'ai deux fonctions qui s'appellent `start_timer` et `show_timer`. Je vous les montre en action. Le principe, c'est juste de pouvoir mettre un *tag* au moment où je démarre l'expérience, puis ensuite avec `show_timer`, je vais pouvoir vous montrer exactement le temps qu'il s'est passé depuis le début de l'expérience, donc c'est extrêmement simple.

Je vous rappelle que dans le tout premier exemple, nous avons vu déjà une boucle, bien entendu, et on avait utilisé pour ça une fonction qui s'appelle `run_until_complete` donc c'est une fonction qui prend exactement un argument, et on l'avait utilisée pour faire du parallélisme en conjonction avec `asyncio.gather`, qui, je vous le rappelle, va se charger d'exécuter plusieurs tâches en parallèle et de nous retourner la liste des résultats. Il y a d'autres façons d'utiliser une boucle. Les mécanismes en question sont extrêmement simples, mais j'ai trouvé qu'ils étaient particulièrement mal expliqués dans la documentation, donc je vais vous sous-titrer quelques usages.

Nous avons pour commencer une fonction dans le module `asyncio` qui s'appelle `ensure_future` et sa fonctionnalité consiste à rajouter un traitement à faire dans une boucle. Alors, je dis traitement, on verra qu'en fait ça s'appelle une tâche mais comme je n'ai pas encore expliqué ce que c'est qu'une tâche, on va essayer d'être générique. C'est une fonctionnalité que vous pouvez utiliser avant de commencer une boucle ; vous pouvez aussi l'utiliser en plein milieu de votre traitement, donc vous pouvez le mettre dans le code synchrone, vous pouvez aussi le mettre dans le code asynchrone. Une fois que vous avez rempli votre boucle, vous pouvez utiliser maintenant une méthode sur l'objet boucle qui s'appelle `run_forever`, qui cette fois-ci ne prend pas d'argument, qui va travailler sur le contenu courant, c'est-à-dire en gros les traitements qui ont été ajoutés, et qui bien entendu donc suppose qu'on a au moins une fois utilisé `ensure_future` avant.

Donc je vais vous montrer comment on peut utiliser cette fonctionnalité pour faire quelque chose qui ressemble à ce type de *workflow*, qu'on appelle un *fork*. C'est-à-dire, j'ai une tâche qui se déroule et, en plein milieu de la tâche, je veux rajouter un autre traitement dans la boucle. Donc pour faire ça, je vais commencer par me définir une coroutine, que j'ai appelée `c1`, donc ça va correspondre à la toute première tâche. Cette coroutine, elle fait tout simplement un premier `sleep` d'une seconde, et au bout d'une seconde, pour déclencher le *fork*, ce qu'elle va faire, c'est d'appeler `ensure_future` avec une deuxième coroutine, puis ensuite, elle va de nouveau attendre une seconde, elle va se terminer. Donc j'évalue ma coroutine. La deuxième coroutine, elle est extrêmement simple, elle se contente d'attendre deux secondes, vous avez vu que la deuxième coroutine n'avait absolument rien à faire. Pour démarrer, donc là je vous rappelle, je n'ai fait que définir des coroutines, pour démarrer, je vais ajouter la coroutine `c1` dans ma boucle par défaut. Pour exécuter le scénario, je vais faire `start_timer` pour qu'on ait une bonne vision de ce qui se passe, et ensuite, je vais simplement faire un `run_forever` sur la boucle. Donc, ce qui va se passer, c'est que je vais déclencher `c1` ; au bout d'une seconde, j'arrive à l'événement `"forking"` ; je démarre `c2` ; au bout d'encore une seconde, `c1` se termine ; au bout d'encore une seconde, `c2` se termine. Donc, maintenant, ma boucle, comme je suis dans un mode boucle sans fin, il faut que je l'interrompe, donc dans cette session-là, ce que je vais faire, c'est que je vais envoyer en gros une interruption clavier, en interrompant mon *kernel* puisque là je suis dans un *notebook*.

Voilà une première utilisation de la boucle et de `run_forever`. Bon, à ce stade-là, j'ai une boucle qui a servi, dans laquelle j'ai ajouté des tâches, dans laquelle j'ai fait des choses. Si je ne suis pas très sûr de l'état dans lequel est ma boucle, je vais utiliser la conjonction de deux fonctions, sur lesquelles je reviendrai un peu plus tard, je vais créer une nouvelle boucle et je vais la déclarer comme boucle par défaut puisque vous avez vu tout à l'heure que j'ai fait `ensure_future` sans préciser aucune boucle, donc, quand j'ai fait ça, j'ai utilisé la notion de boucle par défaut. On y reviendra tout à l'heure, pour l'instant, je vous demande de l'admettre. Et je vais vous refaire le même scénario qu'à l'instant mais je vais vous montrer que je peux aussi, à la fin du scénario, plutôt que d'avoir à interrompre, moi, la boucle, parce que je sais qu'elle est terminée, je peux programmer le fait que le scénario est terminé. Et pour ça, je vais utiliser exactement le même scénario ; simplement, à la fin de `c2`, puisque moi, je sais que c'est la fin de `c2`, je peux appeler une fonction synchrone, ce n'est pas une coroutine, je n'ai pas fait `await`, je dis de manière impérative que la boucle doit se terminer, et donc, si j'évalue ce scénario, vous allez voir que je fais exactement la même chose, donc je pars d'une boucle qui est vide, je lui rajoute une coroutine, et lorsque je déclenche ma boucle, tout le scénario se déroule comme prévu et à la fin, on me rend la main, la boucle ne tourne plus, elle s'est arrêtée proprement.

On va voir maintenant comment on peut mélanger un petit peu les deux mécanismes, ça veut dire que vous pouvez aussi bien utiliser `ensure_future` pour ajouter des tâches et ensuite, utiliser `run_until_complete`. La sémantique est assez simple, on va voir un tout petit exemple. Donc, je me réinitialise ma boucle comme je l'ai fait tout à l'heure, je crée un tout petit utilitaire, donc, là, ce que je vais faire, c'est de simuler un job qui prend un certain temps. Je vous rappelle que, dans tous mes exemples, j'utilise la plupart du temps uniquement des `sleep`. Dans la vraie vie, ce que vous allez faire, c'est des coroutines qui font des vraies choses Bon, évidemment, le contenu de ce qui se passe entre le début et la fin de votre coroutine, ça va être lié à tous les événements qui vont avec la requête http ; tout ça, va fonctionner de la manière qu'on a décrite depuis le début. Encore une fois, ne vous bloquez pas sur le fait que je fais des `sleep`, dans votre tête, imaginez que vous avez simplement un traitement avec plein de petites opérations qui vont se dérouler normalement. Ce qu'on cherche à démontrer ici, c'est la logique d'exécution de la boucle.

Sur la base de cet utilitaire, je vais me définir deux petites coroutines, une longue et une courte, donc une qui prend une seconde, une qui prend deux secondes, et mon scénario, ça va être de vous montrer que je peux mettre dans la boucle la coroutine qui est longue, qui va prendre deux secondes, et ensuite, lancer un scénario dans lequel je fais un `run_until_complete` de la première. Je vais faire ça, je vais exécuter les deux tâches, je ne vais pas me concentrer seulement sur la petite tâche que j'ai mise dans `run_until_complete` ; je fais fonctionner toutes les tâches qui sont dans la boucle, mais je vais m'arrêter dès que le sujet, donc la courte, va se terminer donc au bout d'une seconde. Et, une fois que j'ai fait ça, je peux continuer à relancer ma boucle en faisant un `run_forever` cette fois. Et elle va exécuter ce qui reste comme sujet pour elle, c'est-à-dire en l'occurrence la longue, elle va finir d'exécuter la longue ; elle va même le faire instantanément, vous allez voir ici, il y a un délai de 0 seconde, pourquoi, parce que j'ai beaucoup parlé, et pendant ce temps-là, le temps auquel il aurait fallu faire la fin de la section longue est arrivé donc on le déclenche immédiatement. Et là, de nouveau, j'ai la petite étoile qui indique que mon interpréteur est occupé parce que ma boucle, c'est une boucle sans fin, je ne l'ai pas arrêtée et donc je vais devoir l'interrompre moi avec une interruption clavier.

Pour résumer ce qu'on a vu jusqu'à maintenant au sujet de la boucle, on a vu qu'on avait deux façons de la déclencher, on va dire, on a `run_until_complete`, qui prend exactement une coroutine, qui va retourner sa valeur ; son usage, ça va être en particulier, si vous avez un code qui est massivement synchrone, et que vous voulez simplement à un moment donné, comme on l'a fait dans un des exemples, aller chercher un certain nombre de requêtes http, attendre qu'elles soient toutes là, et puis continuer dans un traitement de type synchrone. Dans un usage de ce genre c'est tout-à-fait `run_until_complete` que vous allez avoir envie d'utiliser. Par contre, si vous avez une application qui est massivement asynchrone, vous allez vous mettre plutôt dans un mode où vous allez créer une boucle au démarrage, la mettre dans un mode de boucle sans fin, et avoir la logique de votre application qui ajoute des tâches au fur et à mesure. Cette méthode évidemment ne retourne pas sauf en cas de stop explicite comme on l'a vu tout à l'heure.

Donc pour finir cette vidéo je vais revenir un petit peu sur la fonction qui s'appelle `get_event_loop`. Le modèle mental, je vous ai expliqué que `asyncio` c'était un modèle *singlethread* comme vous faites tourner une boucle dans un seul *thread*, vous avez par contre la possibilité d'avoir plusieurs *threads* dans votre application, et une boucle d'événements par *thread*, ça ne pose aucun problème ; le modèle mental, c'est que vous allez avoir en général une boucle par *thread*, ça ne fait pas vraiment beaucoup de sens d'avoir deux boucles, en tout cas, pas au même moment, donc `get_event_loop`, c'est un espèce d'utilitaire qui est conçu pour que vous puissiez avoir facilement accès à la boucle courante. Qu'est-ce que ça veut dire, la boucle courante ? En règle générale, c'est simplement la boucle qui correspond au *thread* dans lequel vous vous trouvez. Et donc, il y a cette notion, c'est une espèce de globale on va dire sauf que ce n'est pas une variable globale, c'est une fonction pour accéder à cet objet. Et donc `get_event_loop`, sa fonctionnalité c'est de pouvoir facilement accéder à la boucle courante pour faire des opérations du style, on en a vu un exemple à l'instant dans ma coroutine `c2`, tout à l'heure, j'ai voulu arrêter la boucle, il a fallu que j'accède à l'objet boucle et pour faire ça, je peux typiquement utiliser `get_event_loop`.

En compagnons de cette fonction, vous avez les deux fonctions que j'ai utilisées jusqu'à maintenant sans vous les détailler, `new_event_loop`, c'est une fonction qui crée une nouvelle boucle. En règle générale, on n'a pas besoin de l'utiliser si on est dans une application avec un seul *thread* parce que, par commodité dans le *thread* principal la boucle est déjà créée, la boucle par défaut est déjà créée. Si vous voulez vous mettre dans un mode *multithread*, vous serez amené à utiliser `new_event_loop`. Et `set_event_loop`, qui vous permet simplement de déclarer un objet comme étant le nouvel objet par défaut du *thread*.

Pour résumer ce qu'on a vu dans cette vidéo, on a vu qu'avec `ensure_future` vous pouvez ajouter des choses à faire dans une boucle d'événements. Vous avez deux méthodes pour décider quelle est exactement la portée de ce que vous voulez faire faire à votre boucle, est-ce que c'est une seule chose et attendre que ce soit fini, ou bien la faire continuer jusqu'à ce que, par un autre moyen, on lui dise de s'arrêter. Nous avons vu que on avait des petites fonctions utilitaires pour se propager la boucle courante sans avoir besoin de, explicitement, la passer en argument à toutes les coroutines parce que ça, ça donne un code qui est totalement illisible. On peut, si on lit la documentation, avoir envie de faire ça, donc je vous encourage à surtout essayer par tous les moyens de ne pas le faire, ça n'est pas nécessaire. Et notamment, il est question dans la version 3.7 d'améliorer un tout petit peu un certain nombre de bugs qui pourraient exister, mais, de manière générale, je vous invite à ne pas propager vos objets boucles d'une coroutine à l'autre, il faut utiliser ces mécanismes-là. Et on a vu en définitive un utilitaire qui vous permet d'arrêter la boucle quand vous le voulez, ça n'est pas d'un usage extrêmement fréquent, mais, voilà, à titre anecdotique vous savez que ça existe.


# W8-S7 Tâches et exceptions


Dans la vidéo précédente, nous avons vu comment ajouter des traitements dans une tâche et déclencher la **boucle d'événements** pour lui faire faire des choses. Donc, dans celle-ci, nous allons expliciter la notion de **tâche** qui est, on va dire, l'interface entre l'utilisateur et la boucle.

La fonction `ensure_future` que nous avons explicitée dans la dernière vidéo retourne un objet de type `task`. Un objet de type `task` ça sert à quoi ? Ça sert essentiellement à connaître l'état de son traitement, savoir s'il est terminé ou pas, et à mémoriser, dans le cas où c'est terminé, est-ce qu'il s'est bien terminé ou est-ce que, au contraire, il a levé une exception par exemple. Donc je vais vous montrer tout ça sur une petite animation.

Je me mets dans un scénario d'une boucle dans laquelle je vais créer trois tâches. Je vais appeler `ensure_future` qui va m'envoyer un objet `task`, je vais le faire trois fois. Je rentre dans `run_forever`. La boucle, comme on l'a déjà bien compris va sélectionner une tâche, va la faire avancer jusqu'au prochain `yield`, va sélectionner une autre tâche, va la faire avancer jusqu'au prochain `yield`, et caetera ; on a bien compris cette logique. Et puis, je me mets maintenant dans la situation où une de ces tâches va retourner normalement. Donc c'est une coroutine ; la coroutine a fini son travail, elle fait `return` quelque chose donc qu'est-ce qui va se passer ? je vais mémoriser le résultat de ce retour dans l'objet `task` ; je pourrai ensuite y accéder par la méthode `result()`. Et ensuite, ce que va faire la boucle, c'est de marquer cette tâche comme étant terminée ; du coup, elle n'a plus besoin de s'en occuper ; elle passe à une autre tâche. Et maintenant, je me mets dans le cas où cette autre tâche lève une exception, et je me mets dans le cas aussi où cette exception, elle a été attrapée dans la pile dont on part, c'est-à-dire que, en regardant dans les différents *frames* de la pile, je tombe sur une exception `try` qui effectivement capture l'exception ; dans ce cas-là, de manière tout à fait normale, on va continuer l'exception comme s'il ne s'était rien passé, l'exception ne va pas aller jusqu'à la boucle, la boucle ne va même pas voir qu'il y a une exception ; je vais continuer mon traitement normalement, je vais passer à une autre tâche ; et maintenant, je me mets dans le cas de figure où cette autre tâche lève à son tour une exception mais cette fois-ci, dans aucun des *frames* de la pile de cette tâche, il n'y a de clause `try` qui attrape l'exception. Donc, ce qu'on va faire, c'est de mémoriser cette exception dans l'objet `task`, comme on l'a fait tout à l'heure, simplement on le met dans un champ qui s'appelle `exception`. De nouveau, cette tâche, on la marque comme étant terminée, on n'a plus besoin de s'en occuper, et on se retrouve uniquement avec une tâche.

Donc vous avez bien compris que la tâche, ça va être l'objet qui va permettre de communiquer entre l'utilisateur et la boucle d'événements. J'ai bien insisté dans l'animation sur la notion d'exception, et en particulier dans ce cas pathologique où l'exception n'est pas attrapée nativement dans le code asynchrone, parce que c'est une source de souci, évidemment, si une de vos tâches lève une exception et que personne ne va la lire, ça veut dire que très possiblement, il y a eu quelque chose de vraiment pas bien qui s'est passé. C'est une source de bug assez importante ; c'est même, je devrais dire, en matière de développement, quand vous allez commencer à écrire du code un petit peu sérieux, c'est très probablement le premier souci que vous allez avoir. Vous écrivez du code, et ça se passe mal, et vous n'avez aucun retour. Donc il faut bien apprendre à aller explorer les différents composants des tâches. Donc, les bonnes pratiques, c'est autant que possible d'attraper les exceptions dans le corps du code asynchrone, c'est quand même le plus simple, en règle générale, on s'efforce de faire en sorte que les exceptions n'arrivent pas jusqu'au fond de la pile de la coroutine.

Maintenant, dans le cas, qui peut toujours arriver, où il y a une exception que vous n'avez pas prévue, il est quand même assez important en gros, ce que je cherche à dire c'est si vous avez un code qui arrive à maturité, il y a de très fortes chances pour que vous soyez amené à écrire un code de type robustification qui va se charger d'aller bien explorer le contenu de toutes les tâches et bien vérifier qu'aucune exception n'a été levée. Alors, pour faire ça, il y a un certain nombre de fonctionnalités dans la classe `Task` ; donc là, il s'agit de deux méthodes de classe, vous avez `all_tasks` et `current_task`. C'est surtout `all_tasks` qui nous intéresse, donc je vais vous montrer un petit peu comment ça s'utilise ; je crée une coroutine `foo`, je l'ajoute dans ma boucle sans fin. Je vous montre que le résultat est un objet de type `task`. Je vous montre également que je peux maintenant demander quelle est la liste de toutes les tâches qui sont dans la boucle, alors là, de nouveau, il y a cette notion de boucle implicite qui est utilisée ici ; je n'ai pas été obligé de préciser à quelle boucle je m'intéresse parce que c'est la boucle par défaut. La méthode `all_tasks` me renvoie un ensemble, ce n'est pas une liste, c'est un ensemble ; c'est une façon normale de voir les choses puisqu'il n'y aurait aucune raison de mettre deux fois le même objet. Je peux vérifier que l'objet `task` que j'ai créé par `ensure_future` fait bien partie des tâches et, évidemment, si j'essaie de regarder dans ce contexte-là, quelle est la tâche courante, il n'y en a pas puisque je ne suis pas dans une boucle, il n'y a pas de tâche courante.

Pour finir, je vous signale l'existence d'une méthode `cancel` sur les *tasks* ; c'est une méthode qu'on n'a pas forcément besoin d'utiliser quand on commence, mais dès qu'on fait du code un petit peu sérieux, ça fait partie de la vie de tous les jours que d'avoir à annuler des tâches, donc il faut y être préparé. Contrairement à ce qu'on pourrait penser quand je fais un `task.cancel()` je ne dis pas tout bêtement à la boucle de complètement ignorer ma tâche, c'est un petit peu plus sophistiqué que ça. La boucle va redonner la main à la tâche sauf que la prochaine fois où la tâche aura la main, on va lui envoyer une exception qui s'appelle `CancelledError`, donc vous êtes en charge d'attraper cette exception, vous pouvez même l'ignorer si vous voulez, ce n'est pas forcément une pratique recommandée, mais donc c'est pour vous indiquer le contexte général de cette manipulation d'annulation de tâche.

En résumé, ce qu'on a vu dans cette vidéo, c'est qu'une boucle, c'est principalement un ensemble de tâches, alors, de manière plus précise on devrait dire un ensemble de **futures**. Une tâche, c'est un exemple particulier de future, c'est l'exemple de future qui va avec les coroutines. En tant qu'utilisateur on va dire normal de la librairie, c'est la chose à laquelle vous êtes le plus facilement exposé, ce sont les tâches, et les futures sont des abstractions dont on n'a pas besoin en tant qu'utilisateur de base. On a vu également qu'une tâche c'est principalement une pile donc de ce point de vue-là, ça ressemble à une espèce de *thread* applicatif ; ça mémorise un état, ça mémorise le résultat du traitement pour voir si ça s'est bien passé ou pas ; et, je vous rappelle surtout que c'est de la responsabilité de l'appelant ou de l'utilisateur, on va dire, d'aller explorer les différents objets tâches qui sont dans la boucle pour, en tout cas, vous assurer que vous allez lire les exceptions et si vous ne le faites pas, vous risquez d'obtenir un avertissement émis par le *Garbage Collector*. Simplement, il faut être conscient du fait que le *Garbage Collector* va probablement vous donner une information à un moment qui n'est pas le bon c'est-à-dire que c'est très difficile de savoir à quel moment on peut décider que quelque chose ne va pas. Vous n'avez pas encore lu l'exception de la tâche `T`, mais rien ne me dit que vous n'allez pas la lire dans un quart d'heure ou dans deux heures, donc le moment auquel on est capable de vous donner cet avertissement en règle générale, c'est toujours trop tard. Donc il est bien préférable d'avoir une bonne pratique de développement et dans cet esprit-là, je vous conseille d'aller systématiquement lire l'exception qui est liée aux objets tâches.

À bientôt !


# W8-S8 La librairie asyncio


Dans les vidéos précédentes, nous avons vu la **boucle d'événements**, nous avons vu les **tâches** qui nous permettent d'interagir avec la boucle d'événements. Maintenant, nous allons voir le restant du contenu de la librairie `asyncio`.

Outre les boucles d'événements, je vous avais déjà dit que nous avons comme contenu, un certain nombre d'utilitaires de synchronisation sur lesquels je vais dire quelques mots, quelques exemples. Il y a également des fonctionnalités pour interagir avec les processus, typiquement la possibilité de lancer des programmes, écrire ou lire sur les canaux d'entrée/sortie des programmes qu'on a créés, et il y a également des classes abstraites qui s'adressent, principalement, à tout ce qui peut être protocoles réseaux. On va balayer ça rapidement. Je vais utiliser le même ensemble d'utilitaires qu'on avait déjà vu la fois passée. Et je vais vous montrer quelques exemples qui utilisent une **queue**.

Une **queue**, c'est un *design pattern* très répandu ; il y a exactement le même paradigme dans les librairies qui se préoccupent de *threading*. Je vous montre rapidement la documentation. Une **queue**, c'est un objet qui a une certaine taille, donc vous avez un nombre fini de *slots* et dans ces *slots*, vous pouvez lire et écrire de manière asynchrone donc la logique, c'est que vous pouvez écrire tant qu'il y a de la place, et puis une fois que l'ensemble des *slots* est rempli, la prochaine fois que quelqu'un essaie d'écrire ça va attendre, ça ne va pas bloquer dans le sens où on fait de la programmation asynchrone et ça va attendre qu'une place se libère ; il faut donc que quelqu'un fasse une lecture et une fois que cette lecture sera faite, et bien, il y a une place de libre. Donc c'est un système qui vous permet de faire de la synchronisation ; je vais vous montrer deux petits exemples d'utilisation de la queue.

Alors, pour commencer, on va faire le scénario habituel de consommateur et écrivain. Je crée une queue de taille 1, tout simplement, et je vais faire une coroutine qui est un producteur qui, toutes les secondes, va écrire dans la queue. Alors, pour faire ça simplement, je vais faire un `await queue.put`, je vous ai bien expliqué que `put` était une coroutine donc il faut bien que je fasse un `await`, et puis on verra le message s'incrémenter au fur et à mesure. J'évalue ma coroutine. Je fais une deuxième coroutine qui, elle, va, de la même façon faire une boucle sans fin et qui attend en lecture quelque chose dans la queue, si bien que, chaque fois que l'écrivain va écrire, le consommateur va immédiatement écrire ce qui est consommé. Donc ici, j'utilise, de manière assez rudimentaire, la queue. Enfin, le point important est de se souvenir que le consommateur n'a aucune idée du tempo et que vraiment, il se synchronise sur le producteur, là, j'ai choisi d'écrire toutes les secondes mais si j'écrivais le même code avec une durée *random* entre les événements, ce serait exactement la même chose ; le point qu'on veut illustrer ici, c'est que je peux synchroniser des choses qui sont déclenchées dans un producteur, voir exactement le même événement à peu près au même moment du côté de l'autre coroutine.

Maintenant, pour faire tourner mon scénario, je vais tout simplement ajouter mes deux coroutines dans la queue, comme on l'a vu tout à l'heure. Et, pour exécuter le scénario, je vais mettre mon *timer* à zéro et ensuite, je vais tout simplement faire un `run_forever` et vous voyez que, tout simplement, comme on s'y attend, le producteur est complètement synchrone avec le consommateur, plutôt l'inverse.

Dans un deuxième scénario, je vais vous montrer comment je peux également utiliser une queue pour limiter le parallélisme. Donc l'idée, ce serait, typiquement, vous avez un très grand nombre d'url à aller chercher, mais vous ne voulez pas aller les chercher toutes les dix mille en même temps parce que ce n'est pas possible sur votre environnement, vous voulez vous dire je me donne une fenêtre d'une centaine, je vais en faire 100 en parallèle et dès qu'une de ces requêtes se sera terminée, je l'enlève, je mets une autre requête jusqu'à épuisement des cent mille. Donc c'est typiquement le genre de chose qu'on peut faire très facilement avec une queue également. Je me redémarre ma queue pour qu'elle soit bien propre ; je me crée, dans mon exemple, une fenêtre de tir avec 4 emplacements ; et je définis une coroutine qui va faire, principalement, pour se synchroniser, elle va essayer d'obtenir un *slot* dans la queue en écrivant ; ça paraît un peu contre-intuitif mais quand on veut obtenir un *slot*, on écrit puisque c'est comme ça que ce sera matérialisé. Alors, pour illustrer le sujet, je vais créer un certain nombre de jobs dont la durée sera 1, 2, 3, 1, 2, 3, 1, 2, 3 C'est ce que dit cette ligne de code. Et puis, à peu près, c'est tout ; et lorsque le job est terminé, je rends la main en faisant un `get`. Je vous répète, je fais toujours des `asyncio.sleep` ; imaginez que là, j'ai toute une coroutine qui va chercher une requête HTTP exactement comme on l'a vu dans un des exemples ; la logique serait exactement la même ; ce qui m'importe, c'est de voir qu'en encerclant mon code avec un `put` puis ensuite un `get`, je suis capable d'implémenter cette politique de fenêtrage. Je vous signale d'ailleurs que vous pourriez tout à fait implémenter ça avec un *context manager* asynchrone ; en l'occurrence, j'ai voulu garder le code simple. Donc, voilà ce que ça donne si j'exécute mon code. Voilà ce que ça devrait donner en fonction de ce que je vous ai expliqué, le job 0 a une durée de 1, puis de 2, puis de 3, puis de 1, puis de 2, puis de 3, et vous pouvez remarquer qu'effectivement, ce à quoi je m'attends, c'est d'avoir ces 4 jobs exécutés dans la première seconde, ces 4 jobs exécutés dans la deuxième, et caetera. Dans la quatrième seconde, je n'ai plus que deux jobs à faire. Donc voilà ce qu'il devrait se passer et je vous invite à vérifier que c'est bien ce qu'il se passe ici. Je dois interrompre, comme d'habitude, ma boucle pour la raison habituelle que je suis dans une boucle sans fin.

Ces deux exemples vous donnent un aperçu de ce que vous êtes capable de faire avec les mécanismes de synchronisation ; mais bien sûr, au delà de ça, la vraie valeur ajoutée de la librairie *asyncio* n'est pas là. Ce qui nous intéresse vraiment, c'est tout ce qui est la gestion du temps, la gestion des processus externes, l'interaction avec le réseau, l'interaction avec les fichiers, toutes ces fonctions qui sont un peu magiques du point de vue de l'utilisateur dans ce sens que la boucle asyncio va nous permettre de faire les choses au bon moment en les mélangeant exactement de manière optimale, et ça peut paraître un petit peu magique de comprendre comment tout ça, ça fonctionne. En fait, on a déjà vu tous les mécanismes qui sont à l'œuvre ; on a vu en particulier la communication entre la boucle d'un côté et ces agents actifs de l'autre, que ce soit la gestion du temps, la gestion des *sockets*, la gestion des *process*. En fait, ces deux parties du système sont complices l'une de l'autre et communiquent avec ce mécanisme de `send` et de `yield` qu'on a vu, et l'intelligence réelle de la boucle, ça va être d'utiliser au mieux les capacités de l'*operating system* à un niveau très, très bas selon que ce soit sur Unix ou Windows, avec une implémentation à chaque fois ad hoc.

En particulier, sur Unix, vous pouvez vous amuser à regarder les appels système qui s'appellent `signal` et `select`, qui sont des fonctionnalités de bas niveau qu'on peut utiliser en C ; si vous avez déjà eu l'occasion d'écrire du code de ce niveau-là en C, vous pourrez certainement être d'accord avec moi pour dire que c'est particulièrement ardu à écrire ; la plupart du temps, ça demande énormément de soin. Le gros avantage de *asyncio*, c'est de vous offrir un niveau de réactivité qui est comparable à ce que vous pourriez écrire avec un paradigme de ce niveau, une interaction très, très bas niveau avec l'*operating system*, mais avec une interface de programmation qui est totalement, enfin quasiment identique à ce que vous écririez avec du code synchrone. Je n'en dirai pas beaucoup plus de, notamment, `Transport`, `Protocol` et `Stream`, qui sont des classes abstraites de très bas niveau. Ce sont ces fonctionnalités-là qui sont utilisées par les librairies de haut niveau que vous serez amené à utiliser lorsque vous allez commencer à utiliser *asyncio*. Typiquement, si vous faites du *HTTP*, du *ssh*, du *telnet*, de la base de données et enfin, toutes les fonctions, je dirais, de niveau utilisateur, sont implémentées sous forme de librairies avec lesquelles vous allez interagir avec des `async with`, avec des `async for`, enfin, des *context managers* asynchrones, des itérateurs asynchrones, des coroutines. De ce point de vue là, vous n'avez pas besoin, dans un premier temps, de rentrer très profondément dans les internes de *asyncio* ; c'est pourquoi je n'en dirai pas plus aujourd'hui.

Mais sachez que, avec ce mécanisme, vous avez vraiment une méthode optimale d'utiliser votre temps CPU et de tirer le profit maximum de votre machine et d'avoir une application très réactive tout en restant très lisible. Dans la prochaine vidéo, nous verrons une espèce de résumé sur tous les écueils qui se présentent aux débutants, et dans l'immédiat, et bien, je vous dis à bientôt !



# W8-S9 Bonnes pratiques


Nous avons fini de faire le tour de la librairie *asyncio*, de ces mécanismes de programmation asynchrone qui sont offerts par Python, et dans cette dernière vidéo, je vais simplement refaire le tour de toutes les petites choses que j'ai signalées comme étant des sources possibles d'erreur et de frustration.

On va commencer par redire, pour une énième fois, qu'il faut être très attentif à faire la différence entre la fonction coroutine, qui lorsqu'on l'appelle, retourne un objet coroutine, qui ne fait rien, c'est exactement, pour vous souvenir de ce qu'on a dit dans les semaines précédentes, c'est exactement la même chose que si je crée une fonction génératrice et que je l'appelle, et bien, j'obtiens un objet générateur qui peut être utilisé dans une itération mais qui, en soi, n'a rien fait ; le fait d'appeler la fonction, ça ne fait rien. Donc, je vais refaire un petit peu le tour pour que ce soit bien clair pour tout le monde des différentes combinaisons possibles.

Donc je me définis deux objets, une fonction normale, traditionnelle, qui s'appelle `synchro()`, et une coroutine, une fonction coroutine, qui s'appelle `asynchro()`. Et maintenant, je regarde toutes les combinaisons. Je peux depuis une fonction traditionnelle appeler une fonction traditionnelle, donc le cas numéro un ; c'est ce qu'on a fait pendant toutes les semaines de Python, ça, c'est tout à fait normal. On peut imaginer qu'une fonction synchrone appelle une coroutine mais en général ça n'a pas de sens, donc là je vous dis : faites attention, et là par contre, vous n'avez simplement pas le droit d'un point de vue d'erreur de syntaxe de faire un `await` à partir du code synchrone. Ça, c'est la première chose. Donc j'évalue ça pour montrer que ça fait effectivement une erreur de syntaxe. Vous avez aussi, dans l'autre sens, en partant d'un code asynchrone, vous avez tout à fait le droit d'appeler une fonction synchrone ; faites attention simplement à ce que ça ne soit pas bloquant. Vous avez bien entendu le droit d'appeler une coroutine avec un `await`. Vous avez, au sens du langage, le droit d'appeler directement une fonction asynchrone, ou de faire un `await` sur une fonction synchrone donc les utiliser de manière à l'envers, on va dire, mais la plupart du temps, c'est suspect, on va regarder ces cas-là un petit peu plus en détail.

Le cas numéro deux, c'est le cas où une fonction normale appelle une coroutine sans faire de `await` ; je vous donne un exemple, que je vais exécuter dans un *process* externe parce que lorsque je suis dans un *notebook* c'est un petit peu confusant, on n'a pas forcément toujours le même sentiment et même retour que lorsqu'on envoie un vrai programme à Python. Si j'invoque ce petit programme à Python, je vais avoir un avertissement qui me dit que ma coroutine, je l'ai créée mais je ne l'ai jamais *awaitée*. C'est ce qu'on veut ; simplement comme je l'ai déjà dit dans une vidéo précédente, ce message, il arrivera probablement très tard, puisqu'en fait, il n'y a aucun moment où on peut être sûr que vous ne ferez pas un `await` avec cet objet un moment plus tard. J'attire votre attention là-dessus ; le système s'efforce de vous donner un avertissement, simplement, la plupart du temps, ça peut arriver un petit peu à contretemps.

Je fais très rapidement le cas n°7 qui était exactement la même chose mais à partir de code asynchrone, on a exactement le même message ; c'est pour vous montrer, puisque ça n'a pas toujours été le cas en réalité, que, qu'on soit dans une boucle ou non, ça n'a pas d'importance, cette situation-là est notifiée de la même façon au programmeur.

Je vois enfin le cas n°8, j'ai une coroutine qui fait un `await` sur une fonction synchrone traditionnelle. Là à nouveau, on a tous les concepts pour comprendre que ça peut être une phrase légale ; si ma fonction synchrone renvoie un objet qui est un `awaitable`, c'est tout à fait légal ; on en a déjà fait lorsque je vous ai montré les *awaitables*. Simplement, dans la plupart des cas, si vous êtes un utilisateur normal, lambda, et que vous commencez à faire du *asyncio* pour faire une application, il y a de très fortes chances pour que vous ne fassiez pas ça. C'est un cas suspect, je vous signale dans le module `inspect`, que vous avez tout un tas de fonctionnalités pour calculer si un objet est une coroutine, un objet coroutine, un générateur, en particulier, vous avez, dans ce cas précis, une fonctionnalité qui s'appelle `is_awaitable` parce que ce que vous voulez vérifier, c'est que l'objet renvoyé par `synchro` est bien un `awaitable`.

Le deuxième écueil le plus fréquent, c'est d'écrire du code qui est trop bloquant. Alors, quand je dis trop bloquant, ça veut dire que dans une coroutine vous ne rendez pas suffisamment la main, donc je vous refais un exemple pour être bien clair là-dessus. Je vais mettre en parallèle deux coroutines, une qui est, on va dire, un bon citoyen parce qu'elle va rendre la main régulièrement c'est celle qui s'appelle `countdown`, elle va nous écrire un point de temps en temps ; et puis une deuxième qui est, je l'ai appelée `compute` pour simuler le fait que c'est un calcul bloquant. Cette fonction-là utilise uniquement `time.sleep` donc elle ne rend pas la main. Si j'exécute le scénario, vous allez avoir à un moment donné lorsque je rentre dans la fonction `compute` je n'ai plus que des `x` ; on n'a pas du tout réparti le temps de manière homogène entre les deux. Pour adresser ce type de problème, vous pouvez par exemple, bien sûr vous pouvez reconcevoir votre code de manière à être plus... il y a une astuce qui peut rendre service, c'est que vous pouvez faire quelque chose du style `await asyncio.sleep(0)` ; ça veut dire en gros que vous ne perdez pas de temps, simplement vous rendez la main à la boucle dans l'hypothèse où quelqu'un d'autre aurait besoin du processeur. Si je fais cette fois-ci tourner le code, vous voyez que les deux tâches se mélangent bien comme on voudrait le faire.

Enfin, il y a cette difficulté que nous avons déjà évoquée et qui est liée aux exceptions. Je vais vous redonner un exemple. De nouveau, je vais exécuter ce code dans un python séparé pour vous montrer un petit peu le type de retour que vous pouvez imaginer avoir, donc je fais un scénario dans lequel une de mes coroutines lève une exception, là, ça va être un *zero divide*. Si je lance ce programme, là, je vous montre le code, pardon, si je lance le programme, vous allez obtenir un message qui est `Task exception was never retrieved`. Ça, c'est vraiment le type de symptôme qui est susceptible d'amener le plus de frustration quand on commence avec *asyncio*. Pour adresser ce genre de problèmes, l'environnement développement *asyncio* vous offre un certain nombre d'assistances ; notamment, il y a un mode *debug*. Le mode *debug* se propose de vous donner plus de renseignements bien entendu puisque c'est un mode *debug* ; il se propose aussi de vous donner des informations un petit peu plus tôt dans le cycle.

Bon, très clairement, ça vient avec l'habitude tout de même ; au bout d'un moment, il n'y a évidemment plus de problème, mais je pense que pour les gens qui développent du code *asyncio* pour la première fois, ce sont les trois erreurs les plus fréquentes je voulais attirer votre attention là-dessus pour vos premiers pas en *asyncio*. Donc voilà qui conclut ce que je voulais vous dire sur la programmation asynchrone en Python. J'espère vous avoir convaincu que c'est une technologie qui est d'abord intéressante mais aussi utile parce que on a de plus en plus affaire à des programmations de ce type avec beaucoup de sources d'informations de toutes sortes à mélanger. J'espère vous avoir donné les bases pour appréhender ce sujet surtout si vous ne connaissiez pas du tout. Mon objectif, ça n'a pas été forcément de vous donner une connaissance fine de tous les mécanismes, d'autant qu'il y a très probablement des évolutions qui vont arriver dans ce domaine. J'ai vraiment essayé de vous donner le paradigme dans sa généralité notamment parce qu'il est vraisemblable que c'est un sujet qui va pas mal évoluer dans les années qui viennent parce qu'il faut se souvenir que ça n'a commencé que finalement il y a deux ans. On ne peut pas vraiment dire qu'on ait une maturité avérée comme on peut l'avoir dans des tas d'autres domaines. Concernant les évolutions, je voulais vous signaler que rien que sur *asyncio*, il y a un certain nombre de choses qui ont déjà été évoquées ; notamment la possibilité de faire un nettoyage, vous avez peut-être remarqué qu'il y a des méthodes sur les tâches, des méthodes sur les boucles... Bon tout ça a besoin, avec des noms qui ne sont quand même pas non plus très parlants, donc, ça, ça a besoin d'être un petit peu nettoyé. La doc a beaucoup besoin d'être améliorée. Mais, au delà de ça, il est important d'avoir en tête la différence entre le langage et la librairie parce que moi, je ne serai pas surpris si d'autres librairies venaient à concurrencer *asyncio* dans le futur.

Enfin, dans tous les cas, je vous remercie d'avoir suivi cette semaine et je vous dis au revoir.
