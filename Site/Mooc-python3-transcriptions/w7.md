# MOOC INRIA / UCA : Python 3 : des fondamentaux aux concepts avancés du langage

## Arnaud Legout et Thierry Parmentelat 

## Transcriptions des videos de la semaine 7 : L'écosystème data science Python

# W7-S1 Présentation générale

Cette semaine, nous allons parler de l'écosystème *data science* en Python, qui est un écosystème en pleine effervescence et extrêmement actif. D'ailleurs, cet écosystème contribue très largement au succès de cette discipline.

Mais qu'est-ce qu'au juste la *data science* ? Il s'agit d'une discipline qui est à la frontière de trois domaines : la programmation, la statistique et l'expertise domaine. Pour être un data scientist, vous devez maîtriser ces trois domaines. Cependant, il est trop souvent sous-estimé l'impact d'une lacune dans un de ces trois domaines. Regardons ce qu'il se passerait si vous n'aviez pas une de ces trois compétences. Alors supposons que vous ayez des compétences en statistiques et en expertise domaine. Vous êtes là dans ce qu'on appelle l'analyse statistique classique : vous avez des données que vous manipulez avec des outils statistiques. Maintenant, supposons que vous ayez des compétences en expertise domaine et en programmation, mais pas de compétences statistiques. Vous êtes ici dans une zone de danger. En effet, vous allez faire des analyses sur vos données sans pouvoir valider que vos analyses sont statistiquement valides. Et pour finir, supposons que vous ayez des compétences en programmation et en statistiques mais pas d'expertise domaine. Vous êtes de nouveau dans une zone de danger.

À ce propos, le diagramme que j'utilise s'inspire très largement du diagramme de Drew Conway. Drew Conway, dans son diagramme original, a mis le *machine learning* à la frontière de la programmation et de la statistique. Je suis en désaccord avec ce choix parce que ça pourrait vous laisser penser que le *machine learning* est la réponse à une lacune dans une expertise domaine. Or, ça n'est pas du tout le cas. Lorsque vous faites du *machine learning*, vous devez être capable de préparer vos données, de faire une sélection de *features*, et également de choisir votre algorithme d'apprentissage. Or, sans une profonde connaissance domaine, vous n'êtes pas capable de faire des choix appropriés.

Comme vous vous en doutez, cette semaine, nous n'allons pas pouvoir couvrir et vous donner des compétences dans chacun de ces trois domaines. Cette semaine, nous allons principalement nous focaliser sur la partie programmation. Et en particulier, nous allons aborder les briques de base que sont **NumPy** et **pandas** dans l'écosystème data science de Python. Il faut prendre conscience qu'il y a de grandes différences en termes de maturité et en termes de philosophie entre Python et NumPy et pandas.  

En termes de maturité, Python 1.0 a été publié en 1994. C'est donc un langage très ancien et très mature. NumPy 1.0 a été publié en 2006, et le projet pandas n'a quant à lui démarré qu'en 2008. On voit donc que Python est très mature et que NumPy et surtout pandas sont beaucoup moins matures.

En termes de philosophie, il y a également de grandes différences. Python est un langage qui dès le départ a eu comme objectif la simplicité. Évidemment, cette simplicité vient avec un prix à payer, lorsque l'on a un langage extrêmement flexible il y est beaucoup plus difficile de faire certains types d'optimisation. NumPy et pandas dès le départ ont eu pour objectif l'efficacité. Et le prix à payer, cette fois, est une perte de souplesse et une perte de simplicité.

Dans toute cette semaine, nous allons utiliser comme environnement de développement les *notebooks*. En effet, les *notebooks* sont l'environnement de choix pour toute la communauté data science parce qu'ils permettent de faire ce qu'on appelle des *runable papers*. Ce sont des papiers dans lesquels on peut mettre du texte formaté mélangé avec du code et avec lesquels on peut interagir dynamiquement avec nos données.

Ouvrons maintenant un *notebook* pour commencer à regarder la performance de NumPy et pandas. Mon objectif ici est de vous montrer l'intérêt des librairies NumPy et pandas sur deux exemples très simples. Bien évidemment, nous reviendrons très largement sur ces exemples dans la suite. Ici, l'intérêt, c'est juste de voir une première motivation.

Commençons avec NumPy. Pour cela, je vais créer une liste `L` qui va contenir mille éléments. Et je vais créer un tableau NumPy qui va contenir les mêmes mille éléments. On crée un tableau NumPy avec `np.array` à partir, par exemple, d'une liste. Et maintenant, on va calculer le carré de ces éléments. Donc on sait faire ça très bien avec les compréhensions de listes. Regardons un exemple. Je vais faire un `timeit` de `x` au carré pour `x` dans `L`. Et donc ensuite, je vais faire la même chose, je vais faire un `timeit` de `x` au carré pour `x` dans `A`. Donc regardons la performance que j'ai pour la liste. Pour la liste, le `timeit` a pris 328 microsecondes pour calculer le carré de ces mille éléments. Et pour le tableau NumPy, ça a pris 216 microsecondes. Donc on voit qu'on a un gain de performance important dans ce cas-là, autour, on va dire, de 30 % de gain de performance.

Mais la vraie performance que l'on obtient avec les tableaux NumPy, c'est en utilisant un nouveau concept qui n'existe pas en Python. C'est le concept de **vectorisation**. Regardons cet exemple. Je vais refaire mon même `timeit` mais maintenant je vais utiliser cette **opération de vectorisation** : pour prendre les carrés des éléments d'un tableau je n'ai qu'à écrire mon tableau que j'élève au carré, et cette opération va appliquer l'opération carré sur chaque élément du tableau. Exécutons cela, et donc regardons le temps d'exécution de cette opération faite maintenant avec le principe de vectorisation, qui est spécifique à NumPy, et on va voir que ce temps d'exécution est beaucoup plus rapide que ce que l'on obtient avec des compréhensions de listes classiques que ce soit sur des listes, ou sur des tableaux. Effectivement, on est passé de 328 microsecondes à 1,42 microseconde, donc un gain de performance absolument majeur.

Encore une fois, nous reviendrons très largement sur ces notions de **vectorisation**, de création de **tableaux NumPy** ; c'était juste pour vous donner une impression ou un premier aperçu de l'intérêt de NumPy. Maintenant, j'aimerais vous montrer un deuxième exemple pour la librairie pandas.

Donc pour ça, je vais importer un jeu de données, donc ça aussi, nous reviendrons dessus dans une prochaine vidéo, un jeu de données sur les passagers qui ont embarqué sur le Titanic. Donc j'exécute cette instruction qui me permet de charger mon jeu de données et je vais regarder à quoi ressemble ce jeu de données. Si je regarde, en fait, c'est un grand tableau et dans ce grand tableau, j'ai un certain nombre de colonnes. J'ai une colonne "survived" qui me donne si ce passager a survécu ou pas à la catastrophe du Titanic. Ensuite, j'ai une colonne "sex" qui va me dire si ce passager était un homme ou une femme. Et j'ai une colonne "class" qui va me dire dans quelle classe était ce passager. Alors, ce grand tableau, c'est un tableau brut qui contient environ huit cents lignes, et moi, j'aimerais avoir une information un peu plus facile à exploiter ; par exemple, j'aimerais bien connaître le taux de survie des passagers par classe et par sexe, pour voir si j'ai une différence entre les passagers des différentes classes et entre les hommes et les femmes. Alors, évidemment, si on devait faire ça à la main, ça serait un petit peu fastidieux mais en pandas, il y a des fonctions qui nous permettent de faire ce calcul de manière extrêmement simple. Donc regardons cela.

Je vais écrire `t = tit`, c'est mon tableau et je vais appeler la méthode `pivot_table` et dessus, je vais dire la chose suivante, je vais dire : donne-moi sur la colonne *survived* donne-moi la moyenne donc je vais donner la fonction que je vais appliquer pour agréger, donc je dis que c'est la moyenne et ensuite je vais dire qu'est-ce que je vais mettre dans mon résultat. Alors, dans mon résultat, je veux voir un petit tableau condensé sur les lignes que je vais appeler index ; je voudrais avoir la classe et sur les colonnes, j'aimerais avoir le sexe. J'exécute cela, et maintenant je regarde mon tableau ; et en une seule instruction, avec une seule fonction, j'ai un tableau qui va me donner le taux de survie par classe et par sexe, donc je vois que les femmes de première classe ont survécu à 96 %, et les hommes de dernière classe ont survécu autour de 13 %.

Pour finir, dans cette semaine, nous ne couvrirons pas les aspects visualisation mais je veux tout de même vous montrer avec quelle simplicité on pourrait visualiser ce résultat. Je reprends simplement ma table `t`, et je vais faire un `plot` et ici, je vais dire avec le type de `plot` que je voudrais un tableau sous forme de barres et donc, regardons le résultat que j'obtiens, et avec une seule instruction `plot` je vais obtenir mon taux de survie par classe et par sexe. Donc les femmes de première classe ont survécu à plus de 90 % et les hommes de dernière classe ont survécu autour de 13 %.

Pour finir, j'aimerais vous expliquer le rôle des librairies NumPy et pandas. NumPy, c'est la librairie de référence pour manipuler des tableaux en Python. Donc la caractéristique de NumPy, c'est qu'on va créer des tableaux multidimensionnels qui ne contiennent que des objets du même type. Une autre caractéristique de NumPy, évidemment, c'est cette très grande performance avec ses opérations de vectorisation mais nous reviendrons largement dessus. pandas, quant à lui, c'est la librairie de référence qui permet d'ajouter des index aux tableaux NumPy. Ça sert à quoi, ces index ? Ça sert à pouvoir donner des labels aux lignes et aux colonnes des tableaux et ainsi de pouvoir accéder de manière beaucoup plus intuitive et expressive aux éléments de ce tableau. De plus, pandas supporte des opérations que l'on trouve en général dans les bases de données comme les opérations, par exemple, de **jointure**, de **pivot** ou de **regroupement**. Nous couvrirons toutes ces notions dans de prochaines vidéos donc si ces termes vous paraissent un petit peu obscurs, pas d'inquiétude, nous reviendrons dessus.

Dans cette vidéo, nous avons vu que le domaine de la *data science* est un domaine complexe qui demande de nombreuses compétences. Cette semaine, nous parlerons uniquement de l'aspect programmation, et à ce titre, les librairies NumPy et pandas fournissent une solution extrêmement efficace et satisfaisante à ce domaine. Cependant, nous avons vu que NumPy et pandas étaient des projets, des librairies qui n'étaient pas parfaitement matures. Donc cela va venir avec certaines frustrations, notamment en termes d'uniformité et en termes de simplicité, que nous aurons à découvrir au cours des prochaines vidéos. Cependant, c'est ce qui se fait de mieux actuellement dans le domaine donc il faut vivre avec ces limitations et profiter pleinement de la puissance de ces librairies NumPy et pandas.

À bientôt !


# W7-S2 Numpy : le type ndarray

La librairie NumPy est la librairie qui permet de manipuler des **tableaux multidimensionnels** que l'on appelle *ndarray* pour *n-dimensional array*. En NumPy, il y a quatre notions fondamentales à maîtriser : évidemment, la notion de **ndarray**, la notion d'**indexation avancée**, la notion de **vectorisation** et la notion de **broadcasting**. Dans cette vidéo, je parlerai uniquement de la notion de *ndarray*, nous couvrirons les autres concepts dans de prochaines vidéos.

Alors, qu'est-ce que c'est, un tableau NumPy ou un *ndarray* ? J'utiliserai dans la suite le terme tableau NumPy. Un tableau NumPy, c'est un tableau qui est stocké dans une zone de mémoire contiguë et qui ne contient que des éléments du même type. Cela a deux avantages majeurs. Le premier avantage, c'est que vous pouvez écrire des fonctions hyper spécialisées pour un type donné. C'est donc ce que vous fournit la librairie NumPy avec des fonctions spécialisées pour chaque type d'objet que vous pouvez avoir dans un tableau NumPy. Le deuxième avantage, c'est que comme les données sont stockées dans une zone de mémoire contiguë, vous pouvez parcourir un tableau de manière extrêmement efficace.

Ouvrons maintenant un notebook pour commencer à jouer avec les tableaux NumPy.

Avant toute opération, on va commencer par importer le module NumPy. Donc on l'importe toujours de la même manière, `import numpy`, et on le renomme toujours `np`, c'est une convention dans cette communauté. Un tableau NumPy peut avoir une, deux ou plus de dimensions. Pour illustrer ce propos, je vais prendre l'exemple de la fonction `ones` qui permet de créer des tableaux NumPy ne contenant que des 1. Donc regardons un exemple. `np.ones` et à `ones`, je vais lui passer un argument `shape` et je vais lui dire, qui prend comme valeur 3. Donc, ça veut dire quoi ? Ça veut dire qu'il va me créer un tableau à une dimension qui contient trois éléments. Ensuite, je peux tout à fait passer à `shape` non pas un entier, mais un tuple, et dans ce cas-là, ça va définir un tableau dans notre cas qui va avoir deux dimensions donc c'est un tableau à deux lignes et deux colonnes. Regardons cet exemple. J'ai bien un tableau à deux lignes et deux colonnes. Et ensuite, je peux évidemment aller plus loin que ça et définir, par exemple, un tableau deux lignes, deux colonnes et puis une troisième dimension. L'argument `shape` étant le premier argument passé à `ones`, il est optionnel, et donc on peut tout à fait écrire quelque chose de la manière suivante : `a = np.ones(2, 2)` et donc j'ai créé un tableau à deux dimensions avec deux lignes et deux colonnes.

Les tableaux NumPy sont des objets qui sont **mutables**. Ça veut dire que je peux modifier les valeurs des éléments dans certaines cases du tableau. Regardons un exemple. J'ai mon tableau `a`, et je vais dire que je vais changer l'élément de coordonnées 1, 1 et il va maintenant être égal à 18. Vous remarquez la manière que j'utilise pour modifier l'élément dans un tableau : je mets les coordonnées entre crochets et chaque dimension est séparée par une virgule. Il y a plusieurs manières de spécifier un élément dans un tableau NumPy mais on vous recommande d'utiliser cette manière qui est la manière la plus sûre et la plus efficace. Regardons cela, `a[1, 1]` égale 18. J'ai bien donc modifié la case 1, 1 de mon tableau avec une valeur qui vaut maintenant 18.

Alors, attention lorsque je vous dis qu'un tableau NumPy est mutable, il ne faut pas que vous croyiez que l'on peut ajouter des éléments dans ce tableau. Un tableau NumPy est défini par une zone contiguë de mémoire et par conséquent, on n'a pas d'opération du type `append` comme ce que l'on aurait par exemple sur des listes. Donc un tableau, une fois qu'il est créé, est défini par une zone de mémoire contiguë qui est fixée. Une autre caractéristique très importante des tableaux NumPy, c'est qu'ils sont définis, les éléments stockés dans le tableau sont définis par un argument que l'on appelle `dtype`. En général, dans cet argument `dtype`, on va mettre ce qu'on appelle des types scalaires, des entiers, des floats, mais on peut également spécifier des types composites que vous verrez dans les compléments.

Regardons les différents types scalaires que je peux mettre dans un tableau NumPy. Je vous rappelle que dans un tableau NumPy, tous les éléments doivent avoir le même `dtype`. Regardons cela. Dans *np scalaire type*, je vais avoir tous les éléments scalaires qui sont disponibles dans mon tableau NumPy. Je vais avoir des types entiers, *int8*, *int16*, *int32* et *int64*, qui signifient que j'ai des entiers codés sur 8 bits, 16 bits, 32 bits, 64 bits, j'ai des types *unsigned int*, donc des entiers qui démarrent à 0, codés sur 8, 16, 32, 64 bits, des types *float* codés sur 16, 32, 64 bits, des types complexes et d'autres types sur lesquels je reviendrai un peu plus tard.

Donc maintenant, prenons un exemple. Je vais faire `a = np.array` et donc la fonction `array` me permet de convertir typiquement, une liste Python en un tableau NumPy. À ma fonction `array`, je vais lui passer 1, 100 et 110. Et je vais spécifier le *dtype* : `dtype = np.int8`. J'exécute ça et je regarde mon tableau ; mon tableau, c'est un tableau tout à fait normal, un tableau NumPy qui contient 1, 100 et 110, et le type des éléments stockés dans le tableau, ce sont des éléments entiers codés sur 8 bits. Donc maintenant, je peux explorer mon tableau de la manière suivante : je peux regarder le *dtype* des objets stockés dans mon tableau, int8, je peux regarder la taille d'un élément, en bytes, donc ici, les éléments sont codés sur 8 bits, ont donc une taille de 1 byte, et ensuite, je peux regarder avec `nbytes` je peux regarder la taille globale de mon tableau de nouveau exprimée en bytes.

Mais maintenant, regardons le cas suivant. Je vais reprendre un tableau NumPy, mais maintenant, je vais lui passer 1, 4.5 et 128. Et je vais spécifier comme `dtype` toujours `np.int8`. Exécutons cela et regardons ce que l'on obtient. Alors ici, on obtient quelque chose qui peut paraître un peu surprenant. On a toujours notre 1, notre 4.5 a été tronqué, donc ça, ça n'est pas la chose la plus surprenante parce qu'on sait bien que même en Python, lorsque l'on convertit un float en entier, et ici, c'est bien un entier, int8, ce sont des entiers, en général, on tronque la partie décimale. Donc on se retrouve avec 4. Mais la partie la plus surprenante, c'est que mon 128 s'est transformé en -128. Alors, qu'est-ce qu'il s'est passé ici ? En fait, je vous rappelle que le type des éléments stockés dans mon tableau, ce sont des entiers codés sur 8 bits. Ils ne peuvent donc prendre que 256 valeurs. Et en fait, ces entiers vont de -128 à 127. Or ici, j'ai voulu entrer la valeur 128 donc qu'est-ce qu'il se passe ? J'ai dépassé la précision de mon int8. Dans ce cas-là, j'arrive à la dernière valeur, 127, c'est la dernière valeur, je reboucle à la première valeur. Et quelle est la première valeur ? C'est -128. Si dans mon tableau, j'avais donné 129, j'aurais obtenu la deuxième valeur, c'est-à-dire -127.

Alors, il y a une dernière chose importante à connaître sur les tableaux NumPy, c'est qu'il est courant en *data science* d'avoir des valeurs qui sont invalides, que l'on appelle, de manière courante, des valeurs *NaN*, *Not a Number*. Donc regardons cela. Je vais prendre un tableau `a` qui va contenir 1, 2 et la valeur `np.nan`. Et donc maintenant, regardons le `dtype` de cela. Je vois que c'est *float64*. En fait, l'objet `NaN` n'est défini que pour les floats. Par conséquent, vous ne pouvez pas avoir un `NaN` dans un tableau d'entiers. Donc même si j'ai spécifié un entier 1, un entier 2 et `NaN`, ces entiers sont automatiquement convertis en *float64*. C'est une des limitations de NumPy, c'est une des limitations importantes, le fait que l'on n'ait pas de notion de *Not a Number* pour les entiers.

Alors, un autre point important avec les tableaux NumPy, c'est que NumPy peut faire de la conversion silencieuse. Regardons un exemple. `a = np.array` et donc ici, je vais spécifier le tableau 1, 2, `np.nan` et maintenant ici, je vais spécifier le type `np.int32`. J'exécute et qu'est-ce que j'obtiens ici ? J'obtiens une erreur `cannot convert float NaN to integer`. Donc ici, c'est un point très important. Vous avez vu que lorsque je n'ai pas spécifié mon `dtype`, ici, NumPy a fait une conversion implicite de type. Par contre, si je spécifie le `dtype`, dans ce cas-là, la conversion n'est plus faite de manière implicite et une erreur est lancée. C'est pourquoi on vous recommande fortement de toujours spécifier le `dtype` dans un tableau NumPy pour éviter des problèmes liés à des conversions implicites de type.

Pour finir, revenons aux types scalaires dans la catégorie Autres. Regardons cela. Je vais faire `np.sctype` et ici, je vais regarder ceux qui sont dans la catégorie *others*. Donc regardons ce qu'on a dedans. Donc on a `bool`, on a `object`, on a `bytes`, `str` et `numpy.void`. Lorsqu'on voit notamment `bytes` et `str`, on pourrait imaginer qu'en fait, on peut spécifier dans un tableau NumPy des chaînes de caractères de taille quelconque. Ce n'est pas du tout le cas. Regardons un exemple. Je vais créer mon tableau NumPy. qui va contenir les chaînes de caractères `'spam'` et `'bean'`. Et ici je spécifie, donc je vous rappelle maintenant c'est une bonne pratique de toujours contrôler votre `dtype` et donc de le spécifier pour ne pas laisser à NumPy le choix pour vous. Je regarde mon tableau et mon tableau contient bien les chaînes de caractères `'spam'` et `'bean'`. Donc on voit ici que ce sont des chaînes de caractères unicode qui contiennent quatre caractères. Donc le `u`, c'est pour unicode, et le 4, c'est pour quatre caractères. On voit donc ici que automatiquement, NumPy a détecté que la plus grande chaîne faisait 4 caractères et a codé toutes les chaînes de caractères sur quatre caractères. Regardons maintenant ce qu'il se passe si jamais je modifie la taille des chaînes. Par exemple, au lieu d'avoir `'bean'`, je prends `'beans'`. J'exécute et je vois maintenant que NumPy a détecté que ma plus grande chaîne faisait 5 caractères, et par conséquent, toutes les chaînes de caractères sont stockées maintenant sur cinq caractères. Donc d'une manière générale, avec les types qui ont des tailles variables, il est beaucoup plus prudent de spécifier vous-mêmes la taille maximum des chaînes de caractères stockées. Regardons cela. Je vais reprendre mon tableau NumPy ici. Et maintenant, dans mon `dtype`, je peux lui passer un tuple qui va être le type et le nombre d'éléments stockés. Donc ici, je vais stocker deux caractères dans mon tableau. Regardons cela. J'ai bien une chaîne de caractères unicode qui ne contient que deux caractères et donc tout le reste a évidemment été tronqué, je n'ai stocké que cette chaîne de deux caractères.

Pour finir, si on revient à nos fameux types scalaires, *scalaire types*, et que je reprends *others* je vois que dedans j'ai également `object` et `numpy.void`. Je vais aller très rapidement sur ces deux types-là. `object` en fait vous permet de stocker non pas des objets dans votre tableau NumPy, mais des références vers ces objets. En fait, le tableau NumPy ne va maintenant plus stocker le contenu des objets, mais va stocker uniquement des références vers ces objets. C'est quelque chose qu'en général on évite de faire parce que lorsque l'on utilise le type `object`, on a une perte de performance importante sur tout ce qui est opération numérique. Le dernier type, c'est `numpy.void`. `numpy.void` vous permet de stocker des objets de taille prédéfinie mais de type quelconque dans votre tableau NumPy, je n'en parlerai pas plus dans cette vidéo.

Nous avons vu dans cette vidéo la notion de **tableau NumPy**. La chose la plus importante à se souvenir à propos de ces tableaux, c'est que ces tableaux sont stockés dans une zone de mémoire contiguë et qu'ils ne stockent que des éléments qui sont tous de même type. Ce type peut être spécifié avec l'argument `dtype` et on recommande fortement de toujours contrôler le `dtype` pour éviter les conversions implicites. Il est classique en *data science* d'avoir des valeurs qui sont invalides ; sachez qu'en NumPy, les seules valeurs invalides, donc les *Not a Number*, ne sont compatibles qu'avec le type float.

À bientôt !


# W7-S3 Numpy : slicing, reshaping et indexation avancée 


Nous avons introduit dans une précédente vidéo la notion de **tableau NumPy**. Dans cette vidéo, nous allons parler de **slicing** que l'on peut appliquer aux tableaux NumPy, de la notion de **reshaping** qui permet de redimensionner des tableaux NumPy, et de la notion d'**indexation avancée** qui permet de sélectionner des éléments dans un tableau de manière expressive et efficace.

Ouvrons maintenant un notebook pour commencer à jouer avec ces différentes notions.

Commençons par créer un tableau NumPy constitué d'entiers choisis au hasard. Donc ici je vais écrire `a = np.random.randint` donc `random.randint()`, c'est une méthode qui nous permet de générer des nombres aléatoires de 1 à 10 et ici, je vais spécifier la `shape` de mon tableau donc ça va être un tableau 3x3. Ici, c'est quelque chose qui peut être un petit peu agaçant et c'est quelque chose qui est un signe du manque de maturité de NumPy, nous avons vu dans une précédente vidéo que pour `ones`, la fonction `ones`, on utilisait l'argument `shape` pour définir la taille d'un tableau ; ici, dans `random.randint`, on utilise l'argument `size`. En fait, c'est exactement le même sens, c'est pour définir les dimensions de mon tableau. Donc ici, j'exécute ça, je regarde mon tableau et ici, j'ai un tableau 3x3 qui contient des nombres aléatoires allant de 1 à 10.

Donc en NumPy, on peut faire du **slicing**, exactement comme ce que l'on ferait avec des listes, avec la même sémantique. Regardons cela. Je vais prendre `a[1:2:]` et donc c'est exactement le même sens qu'un slicing classique sur les listes, je vais de `i` à `j - 1` par pas de `k`. Mais ici évidemment, comme j'ai deux coordonnées je vais faire un slice sur les lignes et un slice sur les colonnes. Regardons le résultat de ce slice. J'obtiens un nouveau tableau.

Alors le point très important du slicing sur les tableaux NumPy, c'est qu'en fait ce slice ne va pas vous retourner réellement un nouveau tableau, il va uniquement vous retourner une **vue** sur le tableau existant. Donc en fait, on ne crée pas de structure de données supplémentaire. Regardons cela.

Je vais définir `b = a[a:,:2]`. Alors `b` est égal à cela. Je regarde mon sous-tableau `b` et maintenant, je vais dire `a[1, 1] = 35`. Je modifie la valeur de l'élément 1, 1 dans le tableau `a`. Et maintenant, donc évidemment, mon tableau `a` a été modifié, l'élément central vaut maintenant 35, mais si je regarde mon tableau `b`, mon tableau `b` a également été modifié. Pourquoi ? Parce qu'en fait il s'agit simplement d'une vue sur le tableau original. En fait, c'est toujours le même principe : en NumPy, on a une zone contiguë de mémoire et on travaille sur cette zone contiguë de mémoire.

Maintenant, regardons une autre caractéristique des tableaux NumPy, c'est la possibilité de changer leurs dimensions, le **reshaping** avec la méthode `reshape`. Regardons cela. Je vais créer un tableau `a` toujours aléatoire `random.randint(1, 10), size = (4, 4)`. Ici, je crée un tableau 4x4 quatre lignes, quatre colonnes. Et maintenant, je peux redimensionner ce tableau. Comment je fais ça ? Je fais ça de la manière suivante : `b = a.reshape` et ici, je vais donner une nouvelle dimension et il faut évidemment que le nombre d'éléments dans le tableau original soit le même que dans le tableau qui a été redimensionné. Regardons maintenant mon redimensionnement avec huit lignes et deux colonnes. Ici, de nouveau, reshape ne va pas vous retourner un nouveau tableau mais va simplement vous retourner une vue sur le tableau original.

Nous allons maintenant parler de la notion de **tableau booléen**. Donc ici, prenons un exemple, un tableau qui représenterait les températures du mois de mars qui iraient de -5 à 20 degrés. Prenons cet exemple, je vais écrire `mars = np.random.randint(-5,  20)` et ici, `size = 31`. Donc c'est un tableau à une dimension qui contient trente-et-un éléments. J'exécute et je vois bien mon tableau de températures. Maintenant, regardons quelles températures sont strictement positives. Pour cela, je peux utiliser l'opérateur de comparaison `mars > 0` qui va effectuer une **opération vectorisée**. Nous reviendrons sur cette notion de vectorisation dans une prochaine vidéo mais sachez simplement que c'est une opération qui est très efficace et qui va me produire quelque chose que l'on appelle un tableau de booléens. L'élément 15 est strictement positif donc ici, dans mon tableau de booléens, j'ai `True`. L'élément -4 est négatif, donc dans mon tableau de booléens, j'ai `False` et j'applique donc cette comparaison à chaque élément de mon tableau. On peut utiliser, pour les comparaisons, n'importe quel opérateur supérieur, supérieur ou égal, inférieur, inférieur ou égal, égal, donc c'est `==`, ou différent. Ces tableaux de booléens sont extrêmement utiles.

Regardons un autre exemple. Je vais faire `mars == 0` : est-ce que j'ai des températures nulles dans mon tableau ? Ici, je n'en ai pas. Vous pouvez vous demander maintenant, j'ai mon tableau de booléens mais qu'est-ce que j'en fais  de ce tableau de booléens ? En fait, ce tableau de boolénes a deux usages. Le deuxième usage qu'on verra  dans quelques instants, c'est l'**indexation avancée** mais le premier usage, c'est lié à la constatation que `True` vaut 1 et que `False` vaut 0. Regardons à quoi ça nous sert.

Supposons que je me pose la question suivante : pour combien de jours il a fait une température positive en mars ? Je peux le savoir en faisant l'opération suivante : `np.sum(mars > 0). `mars > 0` va me générer un tableau de booléens, qui va être `True` uniquement quand c'est strictement positif, et donc `True` vaut 1, et `False` vaut 0, et donc, en faisant la somme, j'ai le nombre de jours où il a fait une température strictement positive en mars.

Maintenant, imaginons que je me pose une autre question : est-ce qu'il y a eu au moins un jour au mois de mars où il a fait 20 degrés ? Pour cela, je vais utiliser `any` qui va me retourner vrai si j'ai au moins un `True` dans mon tableau de booléens. Ici, je fais `mars == 20` degrés et donc, regardons ça : non, il n'y a eu aucun jour en mars où il a fait exactement la température 20 degrés.

Et regardons un dernier cas : `np.all(mars > 0)` qui va me servir à répondre à la question : est-ce que, pour tous les jours du mois de mars, il a fait une température strictement positive ? La réponse est non.

Donc voyez qu'en manipulant ces tableaux de booléens avec ces opérations d'agrégation on peut déjà répondre à des questions intéressantes. Maintenant, j'aimerais vous montrer comment est-ce qu'on peut combiner les tableaux de booléens entre eux, en utilisant les opérateurs ET, OU et NOT. Regardons un exemple.

Avant d'aller plus loin, je vais créer un tableau jours du mois de mars qui va être égal à `np.arange` qui me permet, comme la fonction *built-in* `range`, de générer une suite d'entiers ici allant de 1 à `mars.size` donc le nombre d'éléments que j'ai dans `mars` dans le mois de mars plus un et ici, je vais spécifier mon `dtype` parce que je vous rappelle que c'est toujours une bonne pratique de maîtriser son `dtype`, ici, je vais mettre un `dytpe` à `int8`. Je regarde jours du mois de mars, qu'est-ce que ça contient ? Et bien, ça contient juste les entiers allant de 1 à 31. Et maintenant que j'ai ce tableau des jours du mois de mars, je peux combiner un opérateur booléen sur ces jours avec un opérateur booléen sur le mois de mars directement. Regardons donc un exemple : Je vais faire `np.sum` qui va me permettre  de répondre à la question suivante : je vous montre ça je fais `mars > 10` combien de jours, au mois de mars, il a fait plus de 10 degrés ? Et je rajoute l'opérateur ET et ici, je vais mettre jours du mois de mars supérieur ou égal à 15 donc ça veut dire quoi ? Je vais calculer le nombre de jours où il a fait plus de 10 degrés pour les jours qui ont été après le 15 mars.

Ici, il faut noter plusieurs choses importantes. Regardons le résultat, c'est 7. Notons plusieurs choses importantes. La première chose, c'est que mon opérateur logique ET ici, c'est un opérateur *bitwise*. En effet, comme je manipule des tableaux de bits, je dois appliquer un opérateur *bitwise* et non plus l'opérateur classique AND, OR ou NOT.

Deuxième chose qu'on remarque, c'est que j'ai entouré par des parenthèses `mars > 10` et `jours_mars >= 15`. Pourquoi ? Parce que l'opérateur ET est le plus prioritaire et par conséquent, si l'on veut que notre opération fasse ce que l'on souhaite il faut obligatoirement entourer ce qu'il y a à gauche et ce qu'il y a à droite de notre opérateur *bitwise* par des parenthèses.

Maintenant que l'on connaît les tableaux de booléens, ça serait intéressant de les appliquer à des tableaux existants pour faire ce qu'on appelle de l'**indexation avancée**. Regardons cet exemple. C'est quelque chose qui permet d'obtenir des sous-ensembles de tableaux de manière extrêmement expressive. Ici, je vais prendre mon tableau du mois de mars et comme indice, je vais lui passer directement un tableau de booléens. Et donc je vais passer `mars > 10`. Que va faire cette opération ? Elle va sélectionner dans le tableau `mars` uniquement les valeurs pour lesquelles la température est strictement supérieure à 10 degrés. J'obtiens donc un nouveau tableau qui est un sous-ensemble du tableau original.

Je peux également évidemment passer comme indice avancé des expressions booléennes combinées avec des opérateurs logiques. Par exemple, je vais passer mars strictement supérieur à 10 degrés et jours de mars supérieur ou égal à 15, donc retourne-moi, dans le tableau `mars`, toutes les températures supérieures à 10 degrés après le 15 mars et donc voici les températures que j'obtiens.
Sur ces opérations d'indexation avancée, je peux également faire de l'affectation. Faisons une hypothèse. Supposons qu'en fait au mois de mars, nous savons qu'il y a forcément des températures positives et que toutes les températures négatives soient simplement dues à des erreurs de mesure avec notre instrument. À ce moment-là, je peux remplacer toutes les valeurs négatives par, par exemple, la moyenne des températures positives. Regardons cet exemple.

Ici, je vais calculer `moyenne = np.mean` donc je vais calculer avec NumPy la moyenne du mois de mars pour lequel les températures sont strictement positives. Ça, ça va donc me donner une moyenne. Et ensuite, je vais faire `mars` pour les températures qui sont strictement négatives donc mes erreurs de mesure, je vais les remplacer par la moyenne. J'exécute. Et maintenant, je vais regarder mon tableau du mois de mars et dans mon tableau du mois de mars, je vois maintenant  que je n'ai plus aucune température négative et les températures négatives ont été remplacées par la moyenne sur les jours sur lesquels il a fait une température positive.

Pour finir de parler de l'indexation avancée, j'aimerais vous montrer une dernière possibilité, c'est la possibilité suivante, ici, je vais créer un tableau  que je vais appeler `countries` qui va être égal à `np.array` et ici, je vais passer le tableau suivant qui va contenir `fr`, `us`, `japon`. Et maintenant, je vais pouvoir construire un nouveau tableau à partir de ces valeurs-là en passant à `countries` une liste d'indices. Et ici, je vais lui passer 0, 0, ça veut dire que je vais avoir deux fois `fr`, je vais lui passer 1, 2 et 0. Donc ça me permet à partir d'un tableau original, `countries`, de récupérer les éléments du tableau et de créer un nouveau tableau dans lequel je peux même avoir des éléments dupliqués. Regardons le résultat de cette évaluation : je vais avoir deux fois `fr`, une fois `us`,  une fois `japon` et une fois `fr`.

Dans cette vidéo, nous avons vu les notions de **slicing** et de **reshaping** qui permettent de générer des **vues** sur des tableaux existants et nous avons surtout vu la notion d'**indexation avancée** à partir de **masques**. C'est une opération  qui est extrêmement puissante et qu'il faut prendre le temps de bien maîtriser.

À bientôt !


# W7-S4 Numpy : vectorisation

Dans cette vidéo, nous allons voir comment appliquer une opération à chaque élément d'un tableau NumPy. Nous savons évidemment faire cela avec ce que l'on connaît en Python comme les *boucles for*, les *compréhensions* ou les *fonctions génératrices*. Mais nous allons voir que pour tirer pleinement parti de la performance des tableaux NumPy, il faut utiliser un nouveau concept que l'on appelle la **vectorisation**.

Ouvrons maintenant un notebook pour commencer à jouer avec cette notion.

Commençons par créer un tableau d'entiers. `a = np.arange(1000)` donc un tableau qui contient mille entiers et maintenant, calculons un polynôme sur ce tableau. Regardons cela, je vais le faire de la manière suivante : `timeit` de, alors je vais le faire avec une compréhension, `x` au carré plus 2 fois `x` moins 1 `for x in a`. Et exécutons cela. Ça, on sait tout à fait le faire, on sait faire des opérations de compréhension, des fonctions génératrices, des boucles for sur les objets itérables et `a` est itérable. Mais la bonne manière de le faire, c'est d'utiliser les **opérations vectorisées**.

Alors, qu'est-ce que c'est, une **opération vectorisée** en NumPy ? Les opérations vectorisées exploitent le fait que les tableaux Numpy sont stockés dans des zones contiguës de mémoire et que tous les éléments stockés ont la même dimension. Il est par conséquent extrêmement efficace de parcourir les différents éléments d'un tableau NumPy. Regardons comment est-ce que j'écrirais ça en opération vectorisée. On va voir que c'est extrêmement intuitif et même plus simple qu'une compréhension de liste.

Je vais directement effectuer mon opération sur mon tableau NumPy. Je vais écrire `timeit` de `a` au carré plus 2 fois `a` moins 1. J'exécute et mon opération va automatiquement se faire sur chaque élément du tableau, donc l'opération élévation à la puissance, la multiplication, la soustraction et on va combiner le résultat dans un nouveau tableau NumPy et je vois qu'ici le gain de performance est absolument majeur, j'ai quasiment un facteur 100.

Alors, tous les opérateurs que l'on utilise classiquement, les opérateurs numériques, le plus, le fois, le divisé, la soustraction, les opérateurs de comparaison, supérieur, inférieur, égal, différent, les opérateurs bitwise, sont tous vectorisés. Et tous ces opérateurs sont systématiquement associés à une fonction NumPy. Alors, en général, on préfère utiliser l'opérateur parce qu'on voit que c'est beaucoup plus intuitif d'écrire `a` au carré plus 2 fois `a` moins 1 plutôt que d'appeler des fonctions, mais par contre, les fonctions permettent de passer des paramètres qui parfois sont utiles ; notamment, dans les fonctions vectorisées, j'ai un paramètre qui s'appelle `out`. Quel est ce paramètre `out` ? Il permet de spécifier dans quel objet je vais écrire le résultat. Par défaut, les fonctions vectorisées produisent un nouvel objet. Mais je peux décider d'écrire dans un objet existant ; je vais donc économiser le temps de création de cet objet, et je vais donc avoir un gain important en termes de mémoire et un petit gain en termes de performance. Regardons un exemple.

Je vais écrire `a = np.arange` de 1 à un million et je vais donner comme `dtype` `np.float64`. Et ensuite, je vais comparer avec mon `timeit` donc je vais faire ici une seule opération, `-r 1 -n 1`, donc ça veut dire que je n'exécute qu'une seule fois l'instruction, et je vais faire un `np.sqrt(a)`, racine carrée. Exécutons ça et je vois que ça a pris 5 millisecondes. Maintenant, je vais refaire la même opération mais ici, je vais spécifier l'argument `out`. Donc ici, je vais écrire `out = a`. Je vais réécrire dans le tableau d'origine et je vois qu'ici, j'ai eu un gain  extrêmement important de performance. Et en plus, je n'ai pas créé un nouveau tableau en mémoire.

Certaines fonctions vectorisées ont une variante à laquelle on peut accéder par `at`. Regardons un exemple. Donc j'ai un tableau `a` et je vais écrire `a[:5]`. J'obtiens les premiers éléments de mon tableau `a`. Maintenant, je peux appeler `np.log.at(a, [2, 4])`. Que va faire cet appel de `log` avec `at` ? En fait, on va appliquer la fonction `log` uniquement aux éléments 2 et 4 de `a` et on va les modifier en place, donc `at` me permet d'appliquer une fonction à certains éléments d'un tableau NumPy et de les modifier en place. Exécutons cette fonction et regardons maintenant ce que me redonne `a[:5]` : on voit bien que les éléments 2, donc cet élément, et l'élément 4 ont bien été modifiés en place ; en fait, j'ai appliqué la fonction logarithme à ces deux éléments-là.

Nous avons vu également que les fonctions vectorisées s'appliquaient élément par élément. En réalité, toutes les fonctions vectorisées ne s'appliquent pas élément par élément, il y a certaines opérations vectorisées qui font des opérations d'agrégation. Lorsque l'on travaille élément par élément, il n'y a pas de notion d'axe, il n'y a pas de notion de dimension. Par contre, lorsque l'on agrège des éléments, là, il y a bien une notion d'axe, on doit savoir si on agrège, par exemple, suivant les lignes ou suivant les colonnes. Regardons un exemple.

Je vais écrire `a =  np.arange(1, 10)` et je vais faire un `reshape(3, 3)`. J'exécute, je regarde mon tableau `a` et j'ai donc un tableau qui fait trois lignes et trois colonnes. Et maintenant, je vais faire un `np.sum(a)`. Si je fais `np.sum(a)`, qu'est-ce que je vais obtenir ? Je vais faire la somme de tous les éléments de `a`. Mais à `sum`, je peux lui passer une notion d'axe, et donc regardons cela ; je peux écrire `np.sum(a, axis = 0)` et donc là, ça va me faire une somme qui va être le long des lignes, et donc en fait, je vais prendre chaque colonne et je vais additionner les éléments de la colonne. Maintenant, prenons une deuxième variante ; je vais écrire `np.sum(a, axis = 1)`, et maintenant, je vais faire ma somme suivant l'autre direction, je vais prendre chaque ligne et je vais additionner les éléments sur les lignes.

Pour finir, j'aimerais revenir sur la notion de *NaN*, que l'on a notamment avec les floats dans les tableaux NumPy. Regardons un exemple. Je vais de nouveau créer un tableau en spécifiant le `dtype` qui est égal à `np.float64`, et je vais faire un `reshape(3, 3)`. Je regarde ce que j'obtiens, j'ai bien un tableau qui contient des *float64*, un tableau à trois lignes et trois colonnes. Maintenant, je vais écrire `a[1, 1] = np.nan`. Donc j'ajoute la valeur `NaN` au milieu de mon tableau, et maintenant, j'ai bien `NaN, NaN`, c'est possible de l'avoir dans un tableau de floats, puisque ça fait partie des floats. Maintenant, qu'est-ce qu'il se passe si je calcule une moyenne de mon tableau ? Regardons cela. En fait, j'obtiens la valeur `NaN`. Pourquoi ? Parce que le `NaN` est contaminant. Comme le `NaN` est *Not a Number*, toutes les opérations faites sur des `NaN` vont produire des `NaN`. Il se trouve qu'en NumPy, les fonctions que l'on a ont toutes une variante qui ignore les `NaN`. Et donc ces fonctions démarrent avec le mot `nan`. Regardons cet exemple. `np.nanmean` c'est la fonction qui va calculer la moyenne sur les éléments de mon tableau en ignorant les valeurs `NaN`. De la même manière, je pourrais faire `nanmax(a)` et je calculerais le maximum de tous les éléments de mon tableau en ignorant les valeurs `NaN`.

Dans cette vidéo, nous avons vu la notion de **vectorisation**. Il s'agit d'un changement important de paradigme par rapport à Python puisque maintenant, pour tirer pleinement parti de la performance des tableaux NumPy, vous devez utiliser ce concept de vectorisation. En pratique, les fonctions vectorisées sont très souvent écrites en C. Heureusement, vous n'avez pas vous-mêmes à écrire des fonctions en C si vous voulez écrire vos propres fonctions vectorisées ; vous pouvez grâce à deux projets phares que sont Numba et Cython écrire vos codes Python natifs et les transformer, de manière extrêmement facile, en fonctions vectorisées. Nous n'en parlerons pas plus dans cette vidéo puisqu'il s'agit d'un sujet avancé.

À bientôt !


# W7-S5 Numpy : broadcasting


Lorsque l'on fait des opérations arithmétiques sur des tableaux NumPy, les opérations sont faites en général élément par élément. Évidemment, pour que ces opérations aient du sens, l'addition, la multiplication, il faut que les tableaux aient les mêmes dimensions. Cependant, il arrive que l'on veuille faire des opérations sur des tableaux de dimensions différentes. Dans ce cas-là, NumPy définit une notion que l'on appelle le **broadcasting** qui permet de donner un sens à ces opérations  entre tableaux de tailles différentes.

Ouvrons maintenant un notebook pour découvrir cette notion.

Commençons par créer deux tableaux. Je vais écrire un tableau `a` qui est `array` de `[1, 2, 3]` et je vais écrire `b` qui est `np.array` de `[5, 5, 5]`. Et maintenant, faisons `a` fois `b` et regardons le résultat. Alors, le résultat, sans surprise, est le résultat de la multiplication élément par élément des deux tableaux, donc j'ai fait 5 fois 1, 5 fois 2, 5 fois 3. Maintenant, créons un troisième tableau, `c` qui est égal à `np.array` de uniquement `[5]`. Et maintenant, faisons `a` fois `c`. Regardons le résultat, j'obtiens le même résultat. Or, `c` n'a pas la même dimension que `a`. Alors qu'est-ce qu'il s'est passé ici ? En fait, NumPy a fait du **broadcasting**, il a propagé la valeur contenue dans le tableau `c` sur chaque élément du tableau `a`, ce qui me permet de faire la même opération que si mon tableau `c` avait été de dimension 3. Évidemment, on n'est pas obligé de créer des tableaux avec un seul élément, et le broadcasting se généralise à l'opération faite simplement avec un entier. Et donc ici, de nouveau, je vais obtenir le même résultat, mon entier 5 a été propagé, broadcasté, sur chaque élément de mon tableau `a`.

Évidemment le broadcasting est plus général que le simple exemple que l'on vient de voir. Le broadcasting s'applique à des tableaux à n dimensions, mais avec deux inconvénients majeurs. Premièrement, lorsque l'on dépasse des tableaux de deux ou trois dimensions, il devient extrêmement difficile de comprendre ce que va faire cette opération de broadcasting. Le deuxième inconvénient, c'est que le broadcasting peut produire des tableaux de très grandes dimensions et par conséquent, poser un problème de mémoire. Dans la suite, je vais regarder uniquement des exemples basés sur des tableaux à deux dimensions.

Commençons par un exemple. Ici, je vais créer un tableau avec une ligne et trois colonnes, 1, 2, 3 et je vais créer un deuxième tableau qui va être un tableau à trois dimensions, qui contient trois lignes et trois colonnes qui ne sont que des 1. Et maintenant, je vais faire une multiplication entre ces deux tableaux. Donc je vais multiplier une ligne par un tableau à trois dimensions. Pour que l'opération de broadcasting soit définie, il faut appliquer la règle suivante : on prend les dimensions en partant de la droite donc ici, 3, 3 et ces dimensions doivent soit être égales soit l'une des deux doit être égale à 1. pour que l'opération de broadcasting soit définie. Donc ici, 3, 3 : les deux dimensions sont égales et ici, 1, 3 : l'une des deux est égale à 1 donc l'opération de broadcasting est définie. Alors, que va faire ce broadcasting ? Il va faire la chose suivante : il va propager ma ligne sur chacune des lignes du tableau 1, 1. Ici, le point important c'est que cette propagation n'est pas faite en créant un nouveau tableau. C'est quelque chose qui est fait de manière dynamique en appliquant la vectorisation donc sans créer de structure de données temporaire. Et maintenant que j'ai un tableau 1, 2, 3, 1, 2, 3, 1, 2, 3 multiplié au tableau 1, 1, 1, 1, 1, 1 et 1, 1, 1, mon opération de multiplication est définie élément par élément et le résultat va donc être, évidemment, ce nouveau tableau, 1, 2, 3, 1, 2, 3, 1, 2, 3.

Prenons maintenant un nouvel exemple. Je vais maintenant définir un tableau qui va contenir trois lignes et une colonne. Et je vais définir un deuxième tableau, qui est toujours mon tableau à trois dimensions, trois lignes, trois colonnes, qui ne contient que des entiers 1. Et je veux de nouveau appliquer cette opération de multiplication. Donc de nouveau, pour que le broadcasting soit défini, il faut que mes dimensions, en partant de la droite, en les alignant par la droite, soient soit les mêmes ou alors que l'une des deux soit égale à 1. Donc ici, j'ai 3, 1, donc l'une des deux est égale à 1, et la deuxième dimension, c'est 3, 3, elles sont égales, donc le broadcasting est défini. Et donc, que va faire mon opération de broadcasting dans ce cas-là ? Elle va propager ma colonne sur les autres colonnes de mon tableau 1, 1. De nouveau, je ne crée pas de tableau temporaire. Et maintenant, je vais pouvoir définir mon opération de multiplication élément par élément. Et donc le résultat, dans ce cas-là, va être une ligne de 1, une ligne de 2, une ligne de 3.

Pour finir, regardons ce dernier exemple. Je vais définir un tableau à trois lignes et une colonne. Et je vais définir un tableau qui va être à une ligne et deux colonnes. Et je vais multiplier ces deux tableaux. Donc regardons si l'opération de broadcasting est définie dans ce cas. Je vais aligner mes dimensions par la droite, et donc, la dimension la plus à droite, c'est 2, 1 ; l'une des deux est égale à 1 donc pour l'instant, le broadcasting est défini. La deuxième dimension, c'est 3, 1 ; une des deux est égale à 1 ; de nouveau, l'opération de broadcasting est définie. Et dans ce cas-là, je vais produire un nouveau tableau qui va avoir pour dimensions le maximum de la dimension des deux tableaux sur chaque axe, donc mon résultat, ça va être un tableau 3, 2. Et donc, que va faire l'opération de broadcasting ? Elle va propager ma colonne 1, 2, 3 sur une deuxième colonne pour correspondre à un tableau 3, 2 donc à trois lignes et deux colonnes ; et elle va propager ma ligne 4, 5 sur trois lignes pour que de nouveau, j'ai un tableau de dimensions 3, 2. Je vous rappelle qu'ici encore, cette opération de broadcasting est faite sans créer de structure de données temporaire donc c'est quelque chose qui est fait de manière dynamique et de manière extrêmement efficace. Maintenant que j'ai deux tableaux de dimensions 3, 2, je peux les multiplier élément par élément et j'obtiens donc le résultat suivant.

Dans cette vidéo, nous avons vu la notion de **broadcasting** qui permet de donner un sens à des opérations sur des tableaux de dimensions différentes. Nous avons vu que l'opération de broadcasting ne créait pas de structure de données temporaire mais évidemment, le résultat de l'opération de broadcasting va créer l'allocation d'un nouveau tableau qui peut être de très grandes dimensions. Par conséquent, il faut faire attention à cette opération qui peut créer des problèmes de gestion de la mémoire. Une autre difficulté avec l'opération de broadcasting, c'est qu'il peut être difficile de donner un sens à du broadcasting sur des tableaux au-delà de deux ou trois dimensions. C'est pourquoi on vous recommande de limiter ce type d'opération à des tableaux de faibles dimensions.

À bientôt !


# W7-S6 Pandas : introduction aux series et aux index


Nous savons que *NumPy* est la librairie de référence en Python pour manipuler des tableaux multi-dimensionnels. Et c'est quelque chose qui est extrêmement courant d'utiliser notamment lorsque l'on fait du calcul matriciel. Cependant, en *data science*, on a très souvent des labels qui sont associés aux données que l'on manipule. Et on aimerait bien être capable de mettre des labels sur les tableaux que l'on manipule. C'est exactement ce que permet de faire *pandas*.

Regardons un exemple. Ici, j'ai un tableau qui représente par exemple des âges, et j'aimerais pouvoir associer chaque entrée dans ce tableau à un prénom. Et bien, en pandas, je peux le faire en rajoutant des labels qui sont représentés par un objet que l'on appelle un index. Et cet objet à une dimension s'appelle en pandas une **Series**. Maintenant, imaginons que j'aie un tableau à deux dimensions, où la première colonne représente des âges et la deuxième colonne représente des tailles, j'aimerais pouvoir rajouter des labels sur les colonnes, Age et Taille, et j'aimerais pouvoir rajouter des labels sur les lignes, qui correspondent à des prénoms. De nouveau, c'est ce que permet pandas avec une structure de données  qui s'appelle le **DataFrame**. Donc en résumé, il y a en pandas deux grandes structures de données, les Series, pour les données à une dimension, et les DataFrames, pour les données à deux dimensions.

Une grande partie de la complexité de pandas vient de la maîtrise de cette notion d'**index**. Un index est un objet qui est très puissant et qui a deux caractéristiques majeures. Il permet un accès optimisé aux données dans le tableau, et il permet également une notion d'alignement automatique lors des opérations. Prenons un exemple, imaginez que vous ayez à additionner deux *DataFrames*, l'opération d'addition ne s'appliquera qu'aux éléments qui ont exactement le même label.

Dans cette vidéo, nous allons principalement parler de la notion de **Series**, et d'**index**. Ouvrons maintenant un notebook pour jouer avec ces deux notions.

Un **index** est un objet immuable qui est à la frontière du *set* et de la liste. Un index contient des éléments qui sont *hashables*, comme un *set*, et il est *sliceable*, comme une liste. De plus, l'index va définir une relation d'ordre sur les éléments qui sont stockés, et il peut contenir des éléments dupliqués. Un index sur une *Series* va permettre d'offrir à la *Series* une interface qui est à la fois une interface de liste et une interface de dictionnaire. En pratique, on crée très rarement nos propres index. En fait, les index vont être créés automatiquement à l'importation de nos données. Et pandas supporte une grande variété de formats de données, aussi bien en import qu'en export. En pandas, on peut importer des données sous le format csv, sous le format json, sous le format html, excel, sql ou pickle. Ce sujet ne présente pas de difficulté particulière et nous le couvrirons dans les compléments.

Regardons maintenant un exemple de série et d'index. Pour cela, je vais commencer par importer pandas et la convention, c'est toujours de le renommer `pd`. Et ensuite, je vais créer une série de la manière suivante. `pd.Series`. Je vais passer à ma série une liste d'éléments, 20, 30, 40, 50, et je vais lui passer un index ; mon index va être des prénoms, par exemple, Eve, Bill, Liz et Bob. Donc, que représente cette série ? Elle représente l'âge de personnes, l'âge de Eve, de Bill, de Liz et de Bob. Alors, explorons cette série.

Dans cette série, je peux récupérer les valeurs, `values`, et donc ça va me sortir le tableau NumPy correspondant aux valeurs hébergées par cette série. Je peux accéder à l'index avec l'attribut `index`, et donc ça va me retourner l'objet index qui est dans cette série. Et ensuite, je peux accéder aux éléments de cet objet. Alors, regardons un exemple. Je vais écrire `s.loc("eve")`. Ça va me permettre d'accéder à la valeur correspondant au label `eve`. Donc vous remarquez ici que j'ai utilisé l'attribut `loc` ; c'est très important  de toujours utiliser cet attribut. Vous verrez dans certaines documentations ou sur internet, qu'il est possible d'accéder à la valeur stockée par `eve` directement de cette manière ; on vous déconseille très fortement d'utiliser cette notation qui a de nombreux effets de bord. Dans la suite, je n'utiliserai  que la notation `loc`.

Donc, je peux accéder à `eve` mais je peux également faire un *slice* ; je peux aller par exemple de `eve` jusqu'à `liz`. Regardons le résultat et j'obtiens les valeurs allant de `eve` jusqu'à `liz`. Il y ici une différence notable avec le *slice* que vous connaissez habituellement, c'est que ici, je fais un *slice* de `i : j` mais je vais de `i` jusqu'à `j` inclus, donc je vais de `eve` jusqu'à `liz` inclus, la borne de droite est effectivement une borne que l'on prend en compte et qui est incluse.

Comme on peut faire des *slices* sur les labels, vous vous demandez sans doute quelle est la relation d'ordre que j'ai sur mes labels. Cette relation d'ordre est déterminée à la création de ma *Series* par l'ordre dans lequel je vais spécifier mes labels lorsque je définis mon index. Regardons un exemple, je vais reprendre le cas précédent, donc je reprends cette série, que je recopie en dessous, mais maintenant, je vais intervertir `bill` et `liz`. J'exécute cette série. Je recrée le même *slice* que celui du dessus, `s.loc` de `eve` à `liz` et quel est le résultat que j'obtiens ? Maintenant, je vais bien de `eve` à `liz` mais avec une relation d'ordre qui est définie par l'ordre des éléments à la création de mon index.

Cependant, il y a un cas pour lequel le *slicing* ne marche pas. Le *slicing* sur les labels ne va pas être défini si vous avez des labels dupliqués et qu'en plus, votre index n'a pas été trié. Donc, dit autrement, si vous n'avez pas de label dupliqué, le slicing fonctionnera toujours, et si votre index a été trié, le slicing fonctionnera toujours. En pratique, on ne contrôle pas les labels puisque les labels sont donnés par notre jeu de données. Par contre, vous pouvez toujours trier votre index. Par conséquent, on vous recommande de toujours le faire. Lorsque vous triez l'index, vous avez la garantie que le *slicing* fonctionnera toujours, et en plus, le tri de l'index améliore de manière significative la performance de cet index. Regardons un exemple.

Pour cela, je vais créer un tableau d'animaux qui va contenir des animaux que possèdent certaines personnes, enfin, certains utilisateurs, et donc je vais avoir chien, je vais avoir chat, je vais avoir chat, chien, et poisson. Et ensuite, je vais avoir un tableau de propriétaires que je vais appeler proprio, et qui va contenir le prénom des propriétaires donc je vais avoir Eve qui a un chien, je vais avoir Bob qui a un chat, Eve de nouveau qui a un chat, Bill qui a un chien, et Liz qui a un poisson. Ici, j'ai mes deux listes. Et maintenant, je vais créer une série à partir de ces deux listes. Donc je vais écrire ça de la manière suivante : `s` égale *Series* et je vais passer comme valeurs les animaux et comme index, ma liste qui est index, pardon, et comme index, je vais passer ma liste qui est la liste des propriétaires. J'exécute, je regarde ma série, dans ma série, je vois bien que j'ai comme index, des prénoms, et comme valeurs, les animaux chien, chat, chat, chien, poisson.

Alors maintenant, essayons de faire un *slicing* sur cette série. Je vais faire `s.loc["eve" : "liz"]`. Alors maintenant, qu'est-ce qu'il va se passer ? On remarque que je suis dans un cas pour lequel le *slicing* ne devrait pas fonctionner. J'ai des labels dupliqués et mon index n'a pas été trié. Vérifions cela. Ici, je remarque bien que j'ai une exception `KeyError`, et si je regarde tout en bas quelle est mon exception, *Cannot get left slice bound for non-unique label : "eve"*. Mon label eve a bien été dupliqué, je ne peux pas faire de *slicing* dessus. Alors, comment résoudre ce problème ? C'est très simple, on l'a vu il y a juste quelques instants, il suffit de trier l'index. Donc ici, avant de faire ce *slice*, je vais faire `s = s.sort_index`. `sort_index` ne fait pas de tri en place, il retourne une nouvelle série avec laquelle l'index a été trié, donc on doit le réaffecter. Et ensuite, maintenant, je peux bien calculer mon *slice*, donc j'exécute ça, et maintenant, je vois que mon *slice* est bien défini, et si je regarde ma nouvelle série, avec l'index qui a été trié, je vois bien maintenant que j'ai bien Bill, Bob, Eve, Eve et Liz.

Pour finir sur cette notion d'**indexation**, il existe sur les séries un autre attribut qui s'appelle `iloc`. `iloc` me permet d'accéder à des attributs non plus par leur label, mais par leur rang dans l'index. `iloc` de 0, ça me permet d'obtenir le premier élément de la *Series*, et `s.iloc[4]` me permet d'accéder au dernier élément de la *Series*. Il y a ici quelque chose d'important à comprendre, c'est que si jamais je fais un *slice* sur `iloc` par exemple, *slice* de 1 à 3, comme il s'agit d'un *slice* sur des indices, je retrouve le comportement classique des *slices*. Ce qui veut dire que je vais de `i` à `j - 1`  par pas de `k` donc ici, `s.iloc[1:3]` va me retourner deux éléments, l'élément 1 et l'élément 2. Donc, en résumé, lorsque je fais un *slice* sur les labels avec `loc`, je vais de `i` à `j` inclus, lorsque je fais un *slice* sur les indices avec `iloc`, je vais de `i` à `j - 1`.

Comme les tableaux NumPy, les *Series* acceptent également la notion d'**indexation avancée**. Regardons un exemple. Je vais prendre ma série `s`, et je vais récupérer tous les éléments pour lesquels la valeur, c'est `chien`. J'exécute et j'obtiens bien la série pour laquelle la valeur est `chien`. Je peux évidemment faire de l'indexation plus sophistiquée, donc je prends `s` égale `chien` ou `s` égale à `poisson`, et donc je vais récupérer la liste, la série qui contient des chiens et des poissons, et je peux au final faire de l'affectation sur cet index avancé, et donc je peux dire ici que tous ceux qui sont qui valent `chien` ou `poisson` sont maintenant remplacés par `autre`. Si je regarde maintenant ma série `s`, je n'ai bien plus que `chat`, et `chien` et `poisson` ont été remplacés par `autre`.

Pour finir, j'aimerais parler de la notion d'alignement d'index qui est une notion extrêmement importante lorsque l'on parle de séries en Python dans pandas. Regardons un exemple. Je vais maintenant créer deux séries. `s1` qui est égale à `Series` de 1, 2, 3 avec un index qui est égal à une liste qui contient `abc`. Et donc je vais créer une deuxième série `s2` qui est égale à, je vais la copier pour gagner un petit peu de temps, et maintenant, je vais remplacer par 5, 6, 7. Et ma série maintenant va contenir mon index va contenir `a`, `c` et `d`. J'ai donc deux séries `s1` et `s2`, `s1` a pour index `a`, `b`, `c` et contient la valeur `1, 2, 3`, `s2` a pour index `a, c, d` et contient la valeur `5, 6, 7`. Regardons maintenant ce qu'il se passe si je fais une addition. `s1` plus `s2`. Comme je vous l'ai expliqué, pandas va automatiquement aligner les labels, c'est-à-dire que l'opération ne sera définie que pour les valeurs qui ont le même label à gauche et à droite. Exécutons cela. Je vois bien que j'ai bien un `a` dans `s1` et dans `s2`, je fais la somme, ça me donne 6 ; je n'ai `b` que dans `s1` et par conséquent, l'addition n'est pas définie et le résultat, c'est `NaN`, *Not a Number* ; `c` est bien défini dans `s1` et dans `s2`, le résultat, c'est 9 ; et pour finir, `d` n'est défini que dans `s2` et pas dans `s1`, et par conséquent, de nouveau, le résultat, c'est `NaN`, *Not a Number*. Donc lorsqu'il me manque une valeur pour un label donné dans une des deux séries, le résultat de l'opération sera `NaN`. Vous remarquez également que mes deux séries, ce sont des séries qui sont des *int64*. Et que le résultat, c'est un *float64*. Pourquoi ? Et bien parce qu'il y a des opérations qui ne sont pas définies, je dois représenter l'opération non définie par un `NaN`, et une limitation de NumPy, c'est que le `NaN` n'existe que pour les floats donc tout est converti en *float64*. On peut évidemment contrôler ce comportement avec la méthode `add`. `s1.add(s2)` va me donner exactement le même résultat ; mais maintenant, je peux passer un argument qui s'appelle `fill_value` et qui va consister à dire : s'il me manque un élément à gauche ou à droite, je vais le remplacer par une valeur par défaut. Et donc ici, la valeur par défaut qu'on va mettre, c'est 50. J'exécute cela et je vois bien que l'élément manquant a été remplacé par 50 ; maintenant, j'ai pu donner un sens à mon addition. Vous remarquez cependant que bien que `s1` et `s2` soient des séries d'entiers et que ma `fill_value` soit également un entier, le résultat est un *float64* ; il s'agit d'une limitation de la méthode `add` avec l'attribut `fill_value`.

Dans cette vidéo, nous avons vu la notion d'**index** et de **série**. Les index sont des objets très puissants qui donnent aux séries à la fois une interface de liste, donc on peut faire du slicing, et une interface de dictionnaire, on peut accéder à des éléments d'une série à partir d'un label. Nous avons également vu que pandas faisait de l'alignement automatique des labels lorsque l'on fait des opérations sur les séries.

À bientôt !


# W7-S7 Pandas : le type DataFrame

Dans une précédente vidéo, nous avons vu la notion de **série** et d'**index**. Les **séries** sont des tableaux à une dimension sur lesquels on va mettre un **index**. Dans cette vidéo, nous allons voir la notion de **dataframes**, qui sont des tableaux à deux dimensions sur lesquels on va mettre des index sur les lignes et sur les colonnes. Tout ce que nous avons vu sur les séries que ce soit `loc`, `iloc`, le *slicing* ou l'alignement des labels reste valable avec les *dataframes*.

Ouvrons maintenant un notebook pour jouer avec ces objets.

Il existe plusieurs manière de créer une *dataframe* et je vais en créer une à partir de séries. Regardons un exemple ; je vais créer une liste de prénoms qui va contenir `'liz'`, `'bob'`, `'bill'` et `'eve'`. Et ensuite, je vais créer une série `age` qui va donc contenir des âges 25, 30, 35, 40 et qui va avoir pour index les prénoms. Alors, j'ai une erreur de syntaxe parce que j'ai oublié ici mon crochet fermant. Voilà, je réexécute. Ensuite, je vais avoir `taille` qui va également être une *Series* et qui va contenir des tailles. Donc ici, je vais avoir 160, 175, 170, 180 et je vais mettre comme index de nouveau les prénoms. Et puis pour finir je vais avoir une série `sexe` que je vais définir de la manière suivante qui va être une liste construite à partir de `'fhhf'` et qui va avoir pour index de nouveau les prénoms. Et maintenant, je peux construire directement ma *dataframe* à partir de ces séries en passant un dictionnaire qui aura comme clés les labels des colonnes et comme valeurs les séries à ajouter.

Regardons cet exemple, je vais dire `df = Dataframe` et je vais passer le dictionnaire suivant `'age' : age, 'taille' : taille`, je n'oublie pas ma virgule, et `'sexe' : sexe`. J'exécute et maintenant, je peux regarder ma *dataframe* et je vais donc la voir apparaître sous ce format-là où j'ai trois colonnes, `age`, `taille` et `sexe`, et j'ai quatre lignes qui correspondent aux prénoms des personnes.

Alors maintenant, explorons cette *dataframe*. Je peux accéder à l'index des lignes en accédant à l'attribut `index`. Je peux accéder à l'index des colonnes en accédant à l'attribut `columns`. Je peux accéder au tableau NumPy sous-jacent, donc qui contient les valeurs, en accédant à l'attribut `values`. Et puis ensuite, je peux parcourir par exemple les toutes premières lignes de mon *dataframe* en utilisant l'attribut `head` qui va parcourir les deux premières lignes. Et je peux accéder aux dernières lignes de ma *dataframe* en utilisant tail qui par exemple va me retourner les deux dernières lignes. Pour finir, si on veut faire une exploration très rapide des propriétés statistiques de *dataframes*, je peux utiliser `describe` qui va me sortir des statistiques sur les colonnes numériques et à `describe`, je peux lui passer des arguments pour même sortir des statistiques sur les colonnes non numériques mais ici par défaut, c'est les colonnes numériques, et je vois que l'âge moyen est 32,5 ; j'ai la déviation standard, le minimum, les percentiles, le maximum, et j'ai la même chose pour la taille, la taille moyenne, c'est 171,25.

Alors, regardons maintenant l'indexation dans les *DataFrames*. Comme je vous l'ai expliqué en introduction, les *DataFrames* se manipulent comme les *Series*, la seule différence, c'est que maintenant j'ai deux dimensions, j'ai des lignes et des colonnes. Regardons cela. Si je fais `df.loc['liz']` je vais accéder, si je ne donne qu'un seul argument, je vais accéder à la ligne correspondant à `'liz'`. Donc ici, je vois bien la ligne correspondant à liz. Si jamais je veux accéder à une ligne et une colonne, je vais spécifier `'liz'` virgule et la colonne à laquelle je veux accéder par exemple, l'âge. Et donc ici, je vais avoir l'âge correspondant à liz. Donc je vois que je peux accéder aux éléments de ma *dataframe* uniquement à partir des labels. Évidemment, si je veux avoir une colonne en entier, je peux tout à fait le faire en utilisant un *slice*, donc je vais faire `df.loc` : ça veut dire : donne-moi toutes les lignes, `, taille` donc ça va me donner la taille pour tous les éléments de ma *dataframe*. Donc de nouveau, avec `loc`, si je n'ai qu'un seul élément passé à `loc`, ça correspond à des lignes ; si je passe deux éléments séparés par une virgule, le premier correspond aux lignes, le deuxième correspond aux colonnes.

Sur un *dataframe*, je peux également faire de l'**indexation avancée**, comme ce qu'on a vu pour les *Series*. Regardons un exemple, `df.loc[:,age] < 32` et donc ça va me sortir tous les éléments de mon *dataframe* pour lesquels la colonne age est strictement inférieure à 32.

Une opération très courante sur les *DataFrames*, c'est d'enlever l'index, donc de transformer l'index en colonne. Regardons un exemple. Je vous remontre mon *dataframe* `df`. Ici, ma colonne, c'est une colonne qui contient des prénoms. Or, ce que j'aimerais avoir, c'est avoir une colonne de prénoms et non pas les prénoms dans un index. Comment est-ce que je peux faire ça ? Je peux faire ça de la manière suivante, `df = df.reset_index`. `reset_index` va prendre mon index et le mettre dans une colonne. J'exécute. Je regarde ce que vaut `df` maintenant, et maintenant, j'ai une nouvelle colonne qui s'appelle `index` qui contient les prénoms. Mais ce que je voudrais maintenant, c'est renommer cette colonne pour ne plus qu'elle s'appelle `index` mais qu'elle s'appelle, par exemple, `prenom`. Donc je peux faire ça avec la méthode `rename`. Donc je vais faire `rename` et à `rename`, je vais lui dire tu fais un renommage sur les colonnes et à `rename`, je lui passe un dictionnaire qui va contenir pour clé l'ancien nom à remplacer, `index`, et pour valeur, le nouveau nom à utiliser, `prenom`. C'est une syntaxe un tout petit peu particulière mais donc on lui passe un dictionnaire qui a pour clé l'ancien nom, enfin, ce qu'on veut renommer, et pour valeur, le résultat. J'exécute `df`, et je vois donc maintenant que ma colonne s'appelle `prenom`.

Et pour finir, je pourrais dire  maintenant, ce que j'aimerais avoir comme index c'est la colonne des âges. Donc ça, je peux le faire de nouveau en faisant `df = df.set_index` et ici, je vais dire mets comme index la colonne des âges. Et donc au final, j'ai obtenu une *dataframe* donc c'est les mêmes valeurs que ce qu j'avais au départ mais qui ont été réordonnées, j'ai pris l'index, je l'ai mis comme colonne, et j'ai pris une autre colonne que j'ai mise comme index. Ce sont des opérations extrêmement courantes dans les *DataFrames* en pandas.

Mais j'aimerais vous montrer la bonne manière de le faire aujourd'hui parce que vous voyez que j'ai utilisé plusieurs étapes pour découper, pour expliquer ce que je faisais, reseter l'index, renommer une colonne, et recréer un index à partir d'une autre colonne. Ce qu'on utilise aujourd'hui c'est une technique différente. Je vais repartir de ma *dataframe* initiale, que je vais reprendre ici. Donc voici ma *dataframe* initiale et ce que j'utilise aujourd'hui, ce sont des méthodes qui sont chaînées de la manière suivante. Je vous montre ça, je vais écrire `df =` et je mets entre parenthèses et vous allez voir pourquoi je mets entre parenthèses, `df.reset_index` je vais à la ligne et je rajoute l'opération suivante `rename(columns =`   et je passe mon dictionnaire que je vais utiliser pour renommer, donc `index : nom` et ensuite, je passe ma dernière opération `set_index(age)`. Exécutons cela, regardons le résultat, je vois que j'obtiens le même résultat. Mais maintenant, j'ai quelque chose qui est plus expressif, parce que je mets, dans une succession d'opérations, les étapes que je vais appliquer de transformation à mon *dataframe*, et donc, pour que cela fonctionne, il faut que toutes ces méthodes retournent un nouveau *dataframe* et c'est ce que font, en général, les méthodes pandas par défaut.

Pour finir avec les *DataFrames*, j'aimerais vous montrer un exemple d'**alignement d'index**. C'est ce que nous allons voir ici. `df1 =` je vais créer un *dataframe* qui va contenir un tableau de 1 à deux dimensions qui va avoir pour index une liste qui va contenir `a`, `b` et qui va avoir pour colonnes une liste qui va contenir `x`, `y`. Évaluons ça, regardons cette *dataframe* `df1`, je l'ai appelée, voilà, et donc j'ai bien cette *dataframe* que je viens de créer. Maintenant, je vais en créer une deuxième que je vais appeler `df2` et qui va avoir une toute petite variation au lieu d'avoir `a`, `b`, je vais prendre comme index `a`, `c`, et au lieu d'avoir `x`, `y`, je vais prendre comme colonnes, `x`, `z`. J'exécute `df2`. Regardons cette nouvelle dataframe `df2` et maintenant, faisons la somme `df1` plus `df2`. Et qu'est-ce que je vois ? Je vois que j'ai eu un alignement d'index. Lorsque j'ai une valeur de même label, ligne et colonne, à gauche et à droite, alors j'ai fait la somme. C'est le cas pour le label `a, x`. Mais pour tous les autres, je n'avais pas le label soit à gauche, soit à droite, ou soit dans les deux.

Comme on l'a vu avec les séries, je peux tout à fait contrôler ce comportement avec la méthode `add`. Je peux faire un `df1.add(df2, fill_value = 0)` par exemple. J'exécute et maintenant, je remplace la valeur manquante par 0 mais vous remarquez que j'ai toujours des `NaN`. Pourquoi est-ce que j'ai toujours des `NaN` ? Parce que `fill_value` ne va fonctionner que s'il vous manque soit une valeur à gauche soit une valeur à droite. S'il vous manque les deux valeurs, alors le résultat sera `NaN`. Alors je peux me sortir de ce problème-là de la manière suivante ; mon *dataframe*, je vais le renommer `df`, je l'exécute, donc j'ai bien mon `df` ici qui correspond à ce tableau-là, et maintenant, dans pandas, j'ai des méthodes qui s'appellent `fillna` qui me permettent de remplacer les valeurs mises à `NaN` par une valeur par défaut. Ici, je peux mettre -1. Voici un exemple de résultat. Mon `fillna` m'affiche bien -1 à la place des `NaN` ou alors, je peux utiliser une méthode qui s'appelle `dropna` et qui va me jeter les lignes qui contiennent des `NaN`. Ces méthodes `fillna` et `dropna` ont de nombreuses options que nous verrons dans les compléments.

Dans cette vidéo, nous avons vu la notion de **DataFrame**. Les* DataFrames* sont des tableaux à deux dimensions sur lesquels on a des labels sur les lignes et sur les colonnes. Les *DataFrames* comme les *Series* acceptent `loc`, `iloc`, le *slicing*, l'indexation avancée et l'alignement de labels.

À bientôt !


# W7-S8 Pandas : opérations avancées


Une des grandes forces de pandas est le support d'opérations avancées comme la **concaténation**, le **merge** qu'on peut également appeler **jointure**, le **regroupement** et le **pivot**. Ce sont des opérations que l'on trouve habituellement dans les bases de données et qui sont extrêmement puissantes. Cependant ces opérations peuvent être conceptuellement difficiles et ardues à prendre en main à cause de nombreuses options. Mais elles permettent  en une seule ligne de code de faire des traitements qui sont sophistiqués et qui nous demanderaient autrement des développements longs et fastidieux. Dans cette vidéo, je vais vous les présenter dans leur forme la plus simple et vous verrez les formes plus compliquées dans les compléments.

Ouvrons maintenant un notebook pour commencer à jouer avec ces différentes notions.

Commençons par regarder l'opération de **concaténation**. Pour gagner un petit peu de temps, j'ai déjà entré deux *dataframes* dans mon *notebook* ; je vous laisse le temps de mettre la vidéo en pause pour pouvoir les taper. Regardons mes deux *dataframes* ; j'ai une *dataframe* que j'appelle `df1` qui va contenir deux colonnes, `a`, `b`, et deux lignes, `x`, `y`, et j'ai une *dataframe* `df2` qui contient également deux colonnes, `a`, `b`, et deux lignes, `z`, `t`.

Maintenant, j'aimerais concaténer ces *dataframes* en mettant une *dataframe* en dessous de l'autre, donc en fait, en alignant les colonnes. Comment est-ce que je fais ça ? Je fais ça de la manière suivante. Je vais appeler `concat` ; à `concat`, il faut lui passer une liste de *dataframes* à concaténer. Je vais lui passer `df1`, `df2`. Et ici, je vais obtenir une nouvelle *dataframe* dans laquelle j'ai bien mis mes *dataframes* l'une au dessus de l'autre en alignant les colonnes.

Après, je pourrais vouloir aligner les lignes, donc mettre les *dataframes* l'une à côté de l'autre. Ça, je peux également le faire avec `concat` en lui passant un paramètre `axis`. Regardons ce cas-là. Je vais légèrement modifier mes *dataframes* pour qu'elles puissent bien s'aligner dans le bon sens et donc, ici, je vais dire que mes colonnes, c'est `a`, `b`, `c`, `d` pour pouvoir les mettre dans ce sens-là, les colonnes les unes à côté des autres. Et par contre, je vais pouvoir mettre les mêmes lignes, `x`, `y`, `x`, `y`, pour que les lignes soient bien alignées. Maintenant, je vais les aligner de la manière suivante, donc les concaténer de la manière suivante, je passe toujours ma même liste, `df1`, `df2` et ensuite je vais passer `axis = 1` pour changer l'axe de concaténation. Regardons maintenant le résultat et on voit que mes *dataframes* ont bien été concaténées l'une à côté de l'autre en alignant les lignes.

Regardons maintenant le cas de l'opération **merge** qui est équivalente au **join** que l'on trouve dans les bases de données. Notez cependant que *join* existe également en pandas et *join* est un raccourci vers l'opération *merge* avec quelques options de moins. C'est pourquoi en général on préfère manipuler directement la fonction *merge*.

Maintenant, je vais prendre une *dataframe* qui va contenir deux colonnes, une colonne personnel : Bob, Lisa, Sue, et une colonne groupe qui est le groupe dans lequel ces personnes travaillent : service financier, R & D, et Ressources Humaines. Et je vais créer une deuxième *dataframe*, `df2`, dans laquelle j'ai également ma colonne personnel et je vais avoir comme deuxième colonne une date d'embauche : 2004, 2008, 2014. Regardons maintenant ces deux *dataframes*, donc j'ai ma première *dataframe*, `df1`, qui a une colonne personnel, une colonne groupe ; et une deuxième *dataframe* `df2`, qui a une colonne personnel et une colonne date d'embauche.

Maintenant, qu'est-ce qu'il se passe si on fait une opération de *merge* ? En fait, `merge`, par défaut, va essayer de trouver une colonne qui ait le même label à gauche et à droite, donc dans `df1` et dans `df2`. Et effectivement, j'ai une colonne qui a le même label, c'est la colonne personnel. Ensuite, je vais parcourir ma première *dataframe*, et je vais essayer de trouver est-ce qu'au Bob qui est à droite correspond un Bob qui est à gauche ? Si c'est le cas, je vais créer une entrée dans laquelle je vais avoir Bob, le groupe de gauche, la date d'embauche de droite. C'est ce qu'on appelle l'opération de **jointure**. Ensuite, je vais parcourir le deuxième label, je vais trouver Lisa, et je vais chercher est-ce que j'ai un Lisa à droite ? Oui, j'ai un Lisa à droite. Et je vais donc aligner Lisa, R&D, 2004. Et pour finir, j'arrive à Sue. Est-ce que j'ai un Sue à gauche et à droite ? Oui Et je vais donc aligner, faire la jointure de ces deux tables, je vais aligner Sue, RH et 2014.

Ici, je vous ai présenté la version la plus simple du *merge*, il y a beaucoup d'autres options disponibles que nous couvrirons dans les compléments.

Ouvrons maintenant un notebook pour continuer à jouer avec les deux dernières notions, que sont le **regroupement** et le **pivot**. Pour regarder les opérations de **regroupement** et de **pivot**, je vais utiliser un jeu de données réelles. Pour cela, je vais utiliser la librairie `seaborn` qui est une librairie de représentations graphiques appliquées aux données statistiques mais que je ne vais pas utiliser dans ce contexte, je vais uniquement utiliser sa capacité à télécharger un jeu de données publiques, et notamment, le jeu de données que je vais regarder, c'est le jeu de données des passagers du Titanic. Regardons cela, je vais faire `import seaborn as sns` et ensuite, je vais faire un `load_dataset` du jeu de données titanic et je vais récupérer dans ce jeu de données uniquement trois colonnes, donc je prends toutes les lignes et je ne vais prendre que trois colonnes, la colonne `survived`, la colonne `sex` et la colonne `class`. Je vais évidemment nommer ce jeu de données pour pouvoir le manipuler par la suite, je vais l'appeler `ti` comme titanic. Je l'exécute.

Et maintenant, je vais regarder à quoi ressemble ce jeu de données. Je regarde les premières lignes et je vois que j'ai une colonne `survived`, qui est à 0 lorsque la personne n'a pas survécu au naufrage du Titanic et 1 lorsqu'elle a survécu, `sex`, c'est le sexe de cette personne, homme ou femme, et `class`, c'est la classe dans laquelle cette personne a embarqué. Maintenant, regardons un peu plus ce jeu de données, je vais regarder `shape` qui me permet de voir le nombre de lignes et le nombre de colonnes, j'ai 891 lignes donc c'est les 891 passagers qui ont été recensés dans ce jeu de données, et maintenant, j'aimerais, par exemple, accéder uniquement à la colonne de la survie, la colonne `survived`. Je vais exécuter ça. Voilà, j'ai accès à cette colonne et donc, comme j'ai accès à cette colonne, maintenant, je peux calculer la moyenne sur cette colonne. Je vais rajouter `mean` qui me permet d'appliquer la moyenne à tous les éléments de cette colonne et donc je vois que le taux de survie sur cette colonne le taux de survie des passagers du Titanic, en moyenne, était de 38 %.

Alors, maintenant, ce qui m'intéresserait plutôt, ça serait de voir est-ce qu'il y a une différence de taux de survie entre la première, la deuxième et la troisième classe. Ça, évidemment, je peux le faire, je vais faire ce qu'on a déjà vu, qui s'appelle l'**indexation avancée**, donc au lieu de prendre toutes les lignes, je vais prendre uniquement les lignes pour lesquelles la classe est égale à la première classe. Exécutons ça. Alors, il y a une petite erreur de syntaxe, le égal, c'est un double égal, et donc je vois que le taux de survie pour les passagers de la première classe est de 63 %. Alors, maintenant, j'aimerais bien connaître le taux de survie pour les passagers de la deuxième classe, donc ça, je peux le faire, je peux ici écrire `Second` et donc voir maintenant le taux de survie pour la deuxième classe. Et j'aimerais le faire pour la troisième classe. On voit bien que c'est assez rébarbatif. En fait, ce que j'aimerais réellement faire, c'est regrouper mes passagers par classe et calculer la moyenne pour chacun de ces groupes. C'est exactement ce que me permet de faire l'opération `groupby`, l'opération de **regroupement**. Regardons comment ça fonctionne.

Je vais reprendre mon jeu de données titanic et dessus, je vais appeler la méthode `groupby` où je vais lui dire : fais-moi un regroupement par classe. Comment ça va fonctionner, ce regroupement ? Je vais prendre la colonne `class`, je vais regarder mes éléments uniques dans cette colonne, `First`, `Second`, `Third`, et je vais créer un groupe par élément unique, donc un groupe des passagers de première classe, un groupe de deuxième classe, un groupe de troisième classe. Regardons ce que j'obtiens. J'obtiens un objet `DataFrameGroupBy`, donc un objet `GroupBy`. Cet objet va me permettre d'appliquer des opérations à chacun de ces groupes. Je vais pouvoir appliquer des opérations d'agrégation comme une moyenne, c'est ce qu'on va faire, des opérations de filtrage, ou des opérations de transformation. Dans cette vidéo, on ne va regarder que l'opération d'**agrégation** ; nous verrons dans les compléments d'autres types d'opérations.

Donc maintenant, je vais appliquer l'opération d'agrégation moyenne. Et regardons ce que j'obtiens, j'obtiens un nouveau tableau dans lequel j'ai comme lignes, les classes, première, deuxième, troisième, et j'ai une seule colonne `survived` qui va me donner le taux de survie pour la première classe, pour la deuxième et pour la troisième classe. Vous remarquez ici que, dans mon *dataframe* original, j'avais trois colonnes, j'avais `survived`, j'avais `class` et j'avais `sex`. Or ici, la colonne `sex` a disparu. `groupby` applique un principe qu'on appelle le principe de suppression des nuisances colonnes donc des colonnes nuisibles. Ce sont les colonnes sur lesquelles l'opération que l'on applique, ici, l'opération moyenne, n'a pas de sens. Dans ce cas-là, ces colonnes sont automatiquement supprimées de notre résultat.

Imaginons maintenant que je veuille faire un regroupement non plus uniquement par classe pour connaître le taux de survie pour chaque classe, mais un regroupement par classe et par sexe. Quel est, par exemple, le taux de survie des hommes de deuxième classe ou des femmes de première classe ? Alors, je peux le faire avec `groupby`. C'est très simple. Je vais reprendre mon `groupby` et ici, au lieu de lui passer simplement une colonne, je vais lui passer une liste de colonnes. Donc je vais lui passer `class`  et je vais lui passer la colonne `sex`. J'exécute cela. Et j'obtiens bien un nouvel objet `GroupBy` dans lequel j'ai un regroupement à la fois par classe et à la fois par sexe, et où je peux voir que les femmes de première classe ont survécu à quasiment 97 %, alors que les hommes de troisième classe ont survécu à moins de 14 %.

Cependant, si on regarde cet objet, je vais l'appeler `g`, et si on regarde l'index de cet objet, j'affiche mon objet `g` et je regarde l'index de cet objet, `g.index` j'ai en fait un objet qui a un multi-index. Or, les multi-index, ce sont des objets qui sont un peu moins commodes à manipuler que des index classiques, et qui sont également un peu moins commodes à lire. En fait, ce que j'aimerais vraiment avoir, c'est avoir au final un tableau dans lequel je vais avoir dans les lignes les classes, dans les colonnes, le sexe et comme valeur, le taux de survie pour ces classes et pour ce sexe. C'est exactement ce que me permet de faire l'opération **pivot_table**. En fait, on peut voir `pivot_table` comme une généralisation de `groupby` à deux dimensions. Regardons cela.

Je vais reprendre mon jeu de données du Titanic, c'est le jeu de données que j'ai depuis tout à l'heure, et maintenant, je vais appliquer l'opération `pivot_table` et je vais l'appliquer de la manière suivante. Ici, je vais dire la colonne `survived`, je veux que cette colonne `survived` ce soit la colonne que j'agrège. Je veux appliquer comme fonction d'agrégation la moyenne. Par défaut, c'est la moyenne, mais ici, je vous montre comment est-ce qu'on peut spécifier cette fonction. Ensuite, je veux avoir dans mon résultat comme lignes, je vais avoir la classe et ensuite, je vais avoir comme résultats au niveau des colonnes je vais avoir le sexe. J'exécute cette opération et j'obtiens effectivement un tableau dans lequel j'ai pour lignes, les classes, et pour colonnes, le sexe, et pour valeurs, la valeur agrégée du taux de survie, donc je vois que les femmes de première classe ont survécu à 96 % et les hommes de dernière classe à 13 %.

Dans cette vidéo, nous avons vu des opérations avancées de pandas, les opérations de **concaténation**, de **merge**, de **regroupement** et de **pivot**. Ce sont des opérations difficiles à prendre en main mais extrêmement puissantes. Nous n'avons vu dans cette vidéo que les applications les plus simples donc je vous recommande de travailler les compléments pour parfaitement maîtriser ces différentes opérations.

À bientôt !


# W7-S9 Pandas : gestion des dates et des séries temporelles

Il est courant en *data science* de devoir manipuler des **dates** et en particulier des **séries temporelles**. Nous allons voir dans cette vidéo la gestion des dates et des séries temporelles en NumPy et pandas. Ouvrons maintenant un notebook pour commencer à jouer avec ces notions.

NumPy a deux types pour gérer les dates, un type qui s'appelle `datetime64` pour gérer spécifiquement les dates, et un type qui s'appelle `timedelta64` qui est pour gérer les intervalles de temps. Ces deux objets sont codés sur 64 bits avec un codage qui est très malin. En fait, on peut spécifier à NumPy la résolution temporelle que l'on veut avoir, par exemple, la seconde, la milliseconde, l'année, le jour, et l'encodage va automatiquement s'adapter à la contrainte de 64 bits pour coder le maximum de dates possible. Pour vous donner un exemple de cela, si nous choisissons la résolution de la nanoseconde, une résolution très faible, on est capable de coder toutes les dates allant de 1678 à 2262. Si on a besoin d'un plus vaste éventail de dates on peut prendre par exemple la résolution de la milliseconde qui permet de codes des dates couvrant 600 millions d'années. Donc on voit que c'est un codage extrêmement flexible qui permet une grande facilité d'utilisation.

Maintenant, on va voir comment est-ce qu'on peut créer un objet `datetime64`. Donc pour cela, on va appeler la fonction `datetime64` et on va lui passer une date qui doit être dans un format particulier, le format ISO 8601. On ne va pas rentrer dans le détail de ce format, je vais juste vous donner quelques exemples. Ici, on va l'écrire de la manière suivante : 2018 juin 30. Ça s'écrit comme ça, donc ici, j'ai donné la date qui correspond au 30 juin 2018. J'exécute cela et j'ai donc créé un objet `datetime64` qui correspond au 30 juin 2018. Maintenant, je peux évidemment également spécifier une heure. Donc je vais spécifier l'heure de la manière suivante : je vais dire par exemple 8 heures 35 minutes 23 secondes. Je l'exécute et j'ai créé mon nouvel objet en prenant en compte l'heure. Et pour finir, je peux également spécifier la granularité, dont j'ai parlé tout à l'heure, donc ici, je peux dire par exemple : je vais prendre comme granularité la nanoseconde et donc maintenant, j'ai créé mon nouvel objet avec la granularité qui est de la nanoseconde.

Pour finir, je vais vous montrer une manière de créer des objets `timedelta64`. En fait, un objet `timedelta64` s'obtient automatiquement lorsque l'on va faire des opérations, de différence par exemple, entre deux dates. Ici, je vais faire une opération entre le 30 et le 20 juin à 8h37. J'exécute cela et j'obtiens donc automatiquement un objet `timedelta` qui va me donner la différence, dans ce cas-là en secondes, entre ces deux dates.

En pratique, on ne manipule pas directement des types `datetime64` ou `timedelta64` lorsque l'on fait de la *data science*. On utilise plutôt les types natifs fournis par pandas. pandas fournit trois types d'objets pour manipuler les dates. Ce sont les objets `timestamp`, pour manipuler des dates, `period`, pour manipuler une date associée à une durée, par exemple, le 1er juin et une semaine après le 1er juin, ou alors `timedelta` pour manipuler des intervalles entre deux dates. Ces types pandas ont deux avantages majeurs. Le premier avantage, c'est qu'ils permettent un *parsing* des dates beaucoup plus flexible que la contrainte de l'ISO 8601 que l'on a en NumPy. Un deuxième avantage, c'est qu'on peut créer à partir de ces objets des index. C'est ce que nous allons voir dans la suite.

Dans la suite, je vais uniquement parler de l'objet `timestamp` et de son index associé qui s'appelle `datetimeIndex`. Vous noterez que `datetimeIndex` a peut-être un nom qui n'est pas très bien choisi, on aurait préféré avoir un objet qui s'appelle *timestampIndex*, ça fait partie des petites inconsistances que l'on peut trouver en pandas. De nouveau, `timestamp`, c'est l'objet qui gère les dates, et `datetimeIndex`, c'est l'index que l'on construit à partir des *timestamps*.

En pandas, vous avez une fonction qui est un petit peu magique, qui s'appelle `to_datetime`. Cette fonction `to_datetime`, lorsqu'on lui passe une date, est capable de la *parser* à peu près quel que soit son format, et de fournir un objet `timestamp`, ou alors, si on lui passe une séquence de dates, elle est capable de produire un index automatiquement. Regardons un exemple.

Donc je vais prendre `to_datetime` et je vais lui passer une date dans un format qui n'est pas du tout un format ISO, qui est un format standard, 10 juin alors évidemment, il faut le donner en anglais 10 june 1973 à 8h30. Donc ici, je n'ai pas les :, j'ai un format très classique. Et donc ici, il a bien été capable de créer un objet `timestamp` en *parsant* correctement cette date. Maintenant, si à `to_datetime`, je lui passe non plus une date, mais une séquence de dates, nous allons voir qu'il va créer automatiquement un objet `datetimeIndex`. Regardons cet exemple. Ici, je passe mon 10 juin à 8h30 et je vais lui passer un autre format, 22-JUNE-1973. Donc, je vois que les deux formats sont complètement différents. Alors, j'ai une mauvaise syntaxe parce que j'ai rajouté une parenthèse là où il n'en fallait pas. Je réexécute et on voit que malgré ces formats très différents de la première date et de la deuxième date, `to_datetime` a été capable de les *parser* et de produire l'objet correct correspondant.

Alors on a une autre méthode extrêmement pratique qui s'appelle `date_range`, qui me permet de produire des index de dates. Regardons un exemple de cela. Je vais écrire `index = date_range` et donc ici, je vais lui spécifier une date de départ, avec la même facilité de *parsing* que l'on a avec `to_datetime`. Donc je vais spécifier par exemple 1er janvier 2018. Ensuite, je vais lui spécifier une période, c'est-à-dire le nombre de dates que je veux produire, donc ici, je vais en produire mille, et je vais lui spécifier la fréquence, c'est-à-dire quel est l'intervalle que j'ai entre deux dates. Et ici, je vais lui spécifier la journée, un jour. J'exécute cela, et donc, je vois que ce `date_range` m'a produit un index qui part au 1er janvier 2018, avec mille dates, qui sont toutes séparées d'un jour.

Une des grandes forces de pandas dans la gestion des index de dates, c'est qu'on peut spécifier des fréquences extrêmement flexibles. Ici, je vais vous montrer un autre exemple. Je vais recréer un index. Mais cette fois, au lieu de lui dire que mes dates sont simplement séparées d'un seul jour, je vais donner une fréquence beaucoup plus originale. Je vais donner par exemple 43 heures et 36 minutes. Alors vous noterez qu'ici j'utilise un `t` pour minutes, en fait, on peut utiliser soit `min` soit `t`, le `m` tout seul est réservé pour le mois. Donc ici, je spécifie un nouvel index qui va commencer au 1er janvier 2018 avec mille dates qui vont toutes être séparées de 43 heures 36 minutes. J'exécute. Je regarde mon index et je vois bien que j'ai un nouvel index qui est créé en fonction de ce nouvel intervalle. Et maintenant, je vais créer une série à partir de cet index. On va prendre une série `s` qui va contenir des nombres aléatoires donc je vais mettre des nombres aléatoires allant de 0 à 100. Et ici, je vais dire qu'il m'en faut mille parce que mon index a mille éléments. Et ensuite, je vais dire que mon index c'est l'index que je viens juste de créer. Voici. Maintenant, je viens de créer une série qui a pour index une date et qui a pour valeur un nombre aléatoire.

Alors maintenant, j'aimerais vous montrer la souplesse de manipulation des séries lorsque notre index est une date. Supposons que je veuille maintenant obtenir toutes les dates qui sont en 2018. En fait, je n'ai qu'à mettre ici 2018 et pandas va automatiquement m'extraire toutes les dates qui ont eu lieu en 2018. Maintenant, évidemment, je peux faire quelque chose d'un peu plus fin, et je peux dire par exemple que je veux toutes les dates de décembre 2018. Vous remarquez ici que de nouveau, mon format est extrêmement flexible et que pandas va être capable de faire le *parsing* correct de ce format. Et donc, ici, je vais récupérer toutes les dates qui se trouvent en décembre 2018. Et pour finir, je peux évidemment faire du *slicing*. Je vais pouvoir prendre toutes les dates qui vont de décembre 2018 au 3 janvier 2019. Vous remarquez que là, mes formats sont de nouveau hétérogènes et que pandas sait de nouveau parfaitement faire le *parsing* de ces dates et me sortir les éléments de ma *Series* qui vont de décembre 2018 au 3 janvier 2019.

Pour finir, j'aimerais parler d'une opération qui est l'opération de **rééchantillonnage** qui permet de rééchantillonner des séries qui ont des `datetimeIndex`. C'est quelque chose qui est extrêmement puissant et très pratique à utiliser et qui est dans l'esprit assez proche du `groupby`. On va rééchantillonner des dates, donc on va les regrouper par semaine, par mois, par jour, et on va pouvoir appliquer l'opération à chacun de ces groupes. Je vais vous montrer un exemple. Je vais reprendre ma série de tout à l'heure et je vais faire un `resample`, un rééchantillonnage, sur chaque mois. Donc si je regarde ce que j'obtiens, j'obtiens bien un objet un peu comme le `groupby`, j'obtiens un objet qui s'appelle `resampler`. Et sur cet objet, je peux appliquer une opération, ici, je vais appliquer une opération d'agrégation qui est la moyenne. Et donc on voit que j'ai bien obtenu une moyenne sur chacun des mois donc j'ai rééchantillonné par mois ma série et j'ai calculé la moyenne  sur chacun de ces groupes. `resample` est très, très puissant comme méthode, et on peut notamment spécifier des critères d'agrégation qui sont plus sophistiqués que simplement un jour, un mois, une semaine.

Je vais vous donner un autre exemple. Je pourrais dire : je veux faire un rééchantillonnage par semaine mais en démarrant mes semaines le mercredi. Je fais ça simplement de la manière suivante. Je démarre à W pour semaine et WED pour wednesday, pour le mercredi, et maintenant, je vais calculer une moyenne sur mes groupes qui sont faits par semaine et en démarrant le mercredi. `resample` est une méthode qui est bien documentée, ce n'est pas très courant en pandas donc il faut en profiter. Donc si vous regardez `resample?` vous allez avoir la documentation avec un grand nombre d'exemples qui vont pouvoir vous montrer les différents cas d'usage de cette méthode `resample`.

Cette vidéo conclut notre découverte de l'écosystème *data science* en Python. Il y aurait beaucoup plus à dire que ce que l'on a eu le temps de couvrir en une semaine. On aurait aimé parler des évolutions de pandas, des nouveaux formats de données ou même de la visualisation. Cependant, on a préféré se concentrer sur les briques de base que sont NumPy et pandas. Nous avons également vu que NumPy et pandas sont en rupture en termes de philosophie par rapport à Python. La performance est au coeur de la conception de ces librairies. Nous avons également vu qu'elles sont beaucoup moins matures et que donc, des évolutions fondamentales sont à prévoir dans un avenir proche. Vous devez donc prendre le temps d'acquérir ces nouveaux concepts et cette nouvelle philosophie pour tirer pleinement parti de cet écosystème.

À bientôt !
